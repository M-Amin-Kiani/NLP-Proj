# -*- coding: utf-8 -*-
"""Untitled0_(2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jamHC_tjCzyuu0a1iOj4CHhNLBDOaX-y

# Pip Installs
"""

!pip install --quiet tensorflow==2.12.0 torch==2.0.1+cpu torchtext==0.15.2 \
    gensim==4.3.1 scipy==1.10.1 -f https://download.pytorch.org/whl/torch_stable.html

# پاک‌سازی کامل numpy و نصب نسخه هماهنگ با torchtext و tensorflow
!pip uninstall -y numpy
!pip install -q numpy==1.24.3

!pip install -q "portalocker>=2.0.0"

import shutil
shutil.rmtree('/root/.cache/torch', ignore_errors=True)

!pip uninstall -y portalocker
!pip install -q portalocker==2.7.0

 # now Restart Runtime!

import numpy as np
print(" Numpy version:", np.__version__)

from torchtext.datasets import IMDB
train_iter = list(IMDB(split='train'))
print(" تعداد نمونه‌های آموزش:", len(train_iter))

import tensorflow as tf
import torch
import numpy as np
import gensim
import scipy

print("TensorFlow:", tf.__version__)
print("PyTorch:", torch.__version__)
print("NumPy:", np.__version__)
print("Gensim:", gensim.__version__)
print("SciPy:", scipy.__version__)

import nltk
nltk.download('punkt')
nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

print(" همه چیز بدون تداخل آماده است!")

import torchtext
print("torchtext  آماده استفاده است!")

from nltk.corpus import stopwords
print("stopwords example:", stopwords.words('english')[:5])

"""### ری استارت"""

# ----------------------------
#  1. آماده‌سازی داده‌های اولیه و پیش‌پردازش
# ----------------------------
from torchtext.datasets import IMDB
import random
import re
import nltk
from nltk.corpus import stopwords
from collections import Counter
import numpy as np

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('popular')
nltk.download('all')
stop_words = set(stopwords.words('english'))

"""# ------------------------------------------------- Do Not Run Blows!"""

!pip install tensorflow==2.12

# !pip install --upgrade numpy gensim

!pip install gensim==4.3.1 scipy==1.10.1 --upgrade --force-reinstall

# !pip uninstall -y numpy
# !pip install numpy==1.24.4  # یا یک نسخه پایدار مناسب

# نصب ضروریات
!pip install torchtext --upgrade
!pip install "portalocker>=2.0.0"

# پاکسازی نسخه‌های قبلی ()
!pip uninstall -y torch torchtext

# نصب نسخه‌های سازگار
!pip install torch==2.0.1+cpu torchtext==0.15.2 -f https://download.pytorch.org/whl/torch_stable.html

# ----------------------------
#  1. آماده‌سازی داده‌های اولیه و پیش‌پردازش
# ----------------------------
from torchtext.datasets import IMDB
import random
import re
import nltk
from nltk.corpus import stopwords
from collections import Counter
import numpy as np

# دانلود منابع nltk
nltk.download('punkt')
nltk.download('stopwords')
# پاکسازی کامل کش‌های خراب
nltk.download('popular')
nltk.download('all')

stop_words = set(stopwords.words('english'))

"""# Imports"""

from torchtext.datasets import IMDB

# تبدیل به لیست کامل قابل ایندکس
train_iter = list(IMDB(split='train'))
test_iter = list(IMDB(split='test'))

print(f"تعداد داده‌های آموزش: {len(train_iter)}")
print(f"تعداد داده‌های آزمون: {len(test_iter)}")

#   ایندکس‌گذاری و نمونه‌گیری
for i, (label, line) in enumerate(train_iter[:5]):
    print(f" Label: {label}\n Text: {line[:150]}...\n{'-'*50}")

"""# tuple(label, text)"""

from torchtext.datasets import IMDB
import random

train_iter = list(IMDB(split='train'))
test_iter = list(IMDB(split='test'))

# کاهش حجم: فقط نیمی از آموزش و تست نگه داریم
# random.seed(42)  هر بار رندوم ثابته
train_iter = random.sample(train_iter, len(train_iter) // 2)
test_iter = random.sample(test_iter, len(test_iter) // 2)

print(f"تعداد داده‌های آموزش: {len(train_iter)}")
print(f"تعداد داده‌های آزمون: {len(test_iter)}")

#   ایندکس‌گذاری و نمونه‌گیری
for i, (label, line) in enumerate(train_iter[:5]):
    print(f" Label: {label}\n Text: {line[:150]}...\n{'-'*50}")

print("train===================================================test")

#   ایندکس‌گذاری و نمونه‌گیری
for i, (label, line) in enumerate(test_iter[:5]):
    print(f" Label: {label}\n Text: {line[:150]}...\n{'-'*50}")

"""# preprocess"""

import re
import nltk
from nltk.corpus import stopwords
from collections import Counter

# دانلود منابع nltk
nltk.download('punkt')
nltk.download('stopwords')
# پاکسازی کامل کش‌های خراب
nltk.download('popular')
nltk.download('all')

stop_words = set(stopwords.words('english'))

#  تابع پیش‌پردازش برای یک جمله
def preprocess(text):
    # 1. تبدیل به حروف کوچک
    text = text.lower()
    # 2. حذف تگ‌های HTML
    text = re.sub(r"<.*?>", " ", text)
    # 3. حذف کاراکترهای غیرمتنی
    text = re.sub(r"[^a-z\s]", " ", text)
    # 4. توکن‌سازی
    tokens = nltk.word_tokenize(text)
    # 5. حذف stopwords
    tokens = [w for w in tokens if w not in stop_words]
    return tokens

# پردازش کل داده‌های آموزش و تست
train_tokens = [(label, preprocess(text)) for label, text in train_iter]
test_tokens = [(label, preprocess(text)) for label, text in test_iter]

# نمایش نمونه پیش‌پردازش‌شده
for i in range(3):
    print(f" Label: {train_tokens[i][0]}")
    print(f" Tokens: {train_tokens[i][1][:10]}...\n")

"""# حذف نادر ها"""

from collections import Counter

# train_tokens = [(label, tokens)]  ← خروجی پیش‌پردازش اولیه روی داده‌های آموزش
# test_tokens = [(label, tokens)]   ← خروجی پیش‌پردازش اولیه روی داده‌های تست

#  شمارش فراوانی تمام واژه‌ها در اموزش
all_tokens = [token for _, tokens in train_tokens for token in tokens]
token_freq = Counter(all_tokens)

#  تعریف تابعی برای حذف واژه‌های با فراوانی کم (مثلاً فقط یک بار ظاهر شده‌اند)
def remove_rare_words(tokens, min_freq=2):
    return [token for token in tokens if token_freq[token] >= min_freq]

#  اعمال حذف روی train و test
train_tokens_filtered = [(label, remove_rare_words(tokens)) for label, tokens in train_tokens]
test_tokens_filtered = [(label, remove_rare_words(tokens)) for label, tokens in test_tokens]

#  نمایش چند نمونه‌ی بعد از فیلتر برای بررسی
for i in range(3):
    print(f" Label: {train_tokens_filtered[i][0]}")
    print(f" Tokens (filtered): {train_tokens_filtered[i][1][:10]}...\n")

# بررسی تکرار یک واژه خاص در کل دیتای آموزش
from collections import Counter

all_tokens = [token for _, tokens in train_tokens for token in tokens]
token_freq = Counter(all_tokens)

print("mortally:", token_freq["mortally"])

# بررسی اینکه همه کلمات باقی‌مانده، حداقل ۲ بار در train ظاهر شدن
rare_words = [token for _, tokens in train_tokens_filtered for token in tokens if token_freq[token] == 1]
print("کلمات نادر باقی‌مانده:", rare_words)

"""# ساخت بردار بو با نگاشت تک نمود"""

# ساخت دیکشنری واژگان نهایی
all_tokens = [token for _, tokens in train_tokens_filtered for token in tokens]
vocab = sorted(set(all_tokens))
word2idx = {word: idx for idx, word in enumerate(vocab)}
vocab_size = len(vocab)

import numpy as np

def E1(word):
    vec = np.zeros(vocab_size)
    vec[word2idx[word]] = 1
    return vec

def bow_vector(tokens):
    vec = np.zeros(vocab_size)
    for word in tokens:
        if word in word2idx:  # اطمینان از وجود در دیکشنری
            vec += E1(word)
    return vec

train_bow = [(label, bow_vector(tokens)) for label, tokens in train_tokens_filtered]
test_bow = [(label, bow_vector(tokens)) for label, tokens in test_tokens_filtered]

# نمایش یکی از بردارهای BOW به همراه لیبل
label, vec = train_bow[0]
print(f" Label: {label}")
print(f" BOW vector shape: {vec.shape}")
print(f" Non-zero entries: {np.count_nonzero(vec)}")
print(f" Sample vector (first 20 dims): {vec[:20]}")

"""# خوشه بندی"""

import numpy as np
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
from collections import defaultdict
import random

train_bow = train_bow[:2000]

#  استخراج BOWها از train_bow
labels, vectors = zip(*train_bow)  # train_bow = [(label, bow_vec), ...]
X = np.array(vectors)  # ماتریس ویژگی‌ها

from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(n_components=100, random_state=42)
X_reduced = svd.fit_transform(X)


#   KMeans با k=7
k = 7
kmeans = KMeans(n_clusters=k, random_state=42)
clusters = kmeans.fit_predict(X_reduced)

#   خوشه‌بندی فقط روی داده‌های آموزش انجام
#  تست را وارد کنیم، اطلاعات از آینده نشت پیدا می‌کند (data leakage)
# هدف از خوشه‌بندی، تحلیل یا انتخاب داده برای آموزش مدل بعدی است، نه تست.

#  تعیین بهترین k با silhouette score
silhouette_scores = []
k_range = range(2, 11)
for test_k in k_range:
    km = KMeans(n_clusters=test_k, random_state=42).fit(X_reduced)
    score = silhouette_score(X_reduced, km.labels_)
    silhouette_scores.append((test_k, score))

# نمایش silhouette score
best_k, best_score = max(silhouette_scores, key=lambda x: x[1])

#  تجسم خوشه‌ها با PCA
pca = PCA(n_components=2)
X_2d = pca.fit_transform(X_reduced)

plt.figure(figsize=(10, 6))
for cluster_id in range(k):
    plt.scatter(X_2d[clusters == cluster_id, 0],
                X_2d[clusters == cluster_id, 1],
                label=f"Cluster {cluster_id}")
plt.title("پراکنش BOW جملات آموزش با k=7 (نمایش با PCA)")
plt.xlabel("PCA-1")
plt.ylabel("PCA-2")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
X_tsne = tsne.fit_transform(X_reduced)

plt.figure(figsize=(10, 6))
for cluster_id in range(k):
    plt.scatter(X_tsne[clusters == cluster_id, 0],
                X_tsne[clusters == cluster_id, 1],
                label=f"Cluster {cluster_id}")
plt.title("تجسم خوشه‌ها با t-SNE (k=7)")
plt.xlabel("t-SNE-1")
plt.ylabel("t-SNE-2")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


#  کاهش حجم دادگان: انتخاب یکنواخت از هر خوشه
# تعداد m_i برای هر خوشه = min(5, تعداد اعضای خوشه)
clustered_sentences = defaultdict(list)
for i, vec in enumerate(X_reduced):
    clustered_sentences[clusters[i]].append((labels[i], vec))

def select_uniform(cluster_data, max_per_cluster=5):
    if len(cluster_data) <= max_per_cluster:
        return cluster_data
    return random.sample(cluster_data, max_per_cluster)

reduced_dataset = []
mi_list = []
for cid in clustered_sentences:
    selected = select_uniform(clustered_sentences[cid], max_per_cluster=5)
    reduced_dataset.extend(selected)
    mi_list.append(len(selected))

# خروجی نهایی
summary = {
    "کل جملات اولیه": len(X_reduced),
    "تعداد خوشه‌ها (k)": k,
    "تعداد جمله‌ها در هر خوشه (mi)": mi_list,
    "کل جملات پس از کاهش": sum(mi_list),
    "بهترین k بر اساس silhouette": best_k,
    "امتیاز silhouette": round(best_score, 4),
}

summary

"""### نسخه دستی"""

import numpy as np
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA, TruncatedSVD
import matplotlib.pyplot as plt
from collections import defaultdict
import random

# مرحله اول: آماده‌سازی داده‌ها
train_bow = train_bow[:2000]
labels, vectors = zip(*train_bow)
X = np.array(vectors)

# کاهش ابعاد با SVD
svd = TruncatedSVD(n_components=100, random_state=42)
X_reduced = svd.fit_transform(X)

# تابع محاسبه فاصله اقلیدسی
def euclidean_dist(p1, p2):
    return np.sqrt(np.sum((p1 - p2) ** 2))

# پیاده‌سازی دستی معیار سیلوئت
def silhouette_score_manual(X, labels):
    n = len(X)
    clusters = defaultdict(list)
    for idx, label in enumerate(labels):
        clusters[label].append(idx)

    silhouette_vals = []
    for i in range(n):
        xi = X[i]
        cluster_i = labels[i]
        same_cluster = [j for j in clusters[cluster_i] if j != i]
        a_i = np.mean([euclidean_dist(xi, X[j]) for j in same_cluster]) if same_cluster else 0

        b_i_list = []
        for other_cid, indices in clusters.items():
            if other_cid == cluster_i:
                continue
            dist = np.mean([euclidean_dist(xi, X[j]) for j in indices])
            b_i_list.append(dist)
        b_i = min(b_i_list) if b_i_list else 0

        s_i = (b_i - a_i) / max(a_i, b_i) if max(a_i, b_i) != 0 else 0
        silhouette_vals.append(s_i)

    return np.mean(silhouette_vals)

# یافتن بهترین k با سیلوئت دستی
silhouette_scores = []
k_range = range(2, 11)
for test_k in k_range:
    km = KMeans(n_clusters=test_k, random_state=42).fit(X_reduced)
    score = silhouette_score_manual(X_reduced, km.labels_)
    silhouette_scores.append((test_k, score))
    print(f"k={test_k}, Silhouette Score={round(score, 4)}")

best_k, best_score = max(silhouette_scores, key=lambda x: x[1])
print(f"\nبهترین k: {best_k}, امتیاز سیلوئت: {round(best_score, 4)}")

# اجرای نهایی KMeans با k=7 (برای کاهش داده)
k = 7
kmeans = KMeans(n_clusters=k, random_state=42)
clusters = kmeans.fit_predict(X_reduced)

# تجسم با PCA
pca = PCA(n_components=2)
X_2d = pca.fit_transform(X_reduced)
plt.figure(figsize=(10, 6))
for cluster_id in range(k):
    plt.scatter(X_2d[clusters == cluster_id, 0],
                X_2d[clusters == cluster_id, 1],
                label=f"Cluster {cluster_id}")
plt.title("پراکنش BOW جملات آموزش با k=7 (PCA)")
plt.xlabel("PCA-1")
plt.ylabel("PCA-2")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# تجسم با t-SNE
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
X_tsne = tsne.fit_transform(X_reduced)
plt.figure(figsize=(10, 6))
for cluster_id in range(k):
    plt.scatter(X_tsne[clusters == cluster_id, 0],
                X_tsne[clusters == cluster_id, 1],
                label=f"Cluster {cluster_id}")
plt.title("تجسم خوشه‌ها با t-SNE (k=7)")
plt.xlabel("t-SNE-1")
plt.ylabel("t-SNE-2")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# کاهش حجم: انتخاب یکنواخت از هر خوشه
clustered_sentences = defaultdict(list)
for i, vec in enumerate(X_reduced):
    clustered_sentences[clusters[i]].append((labels[i], vec))

def select_uniform(cluster_data, max_per_cluster=5):
    return random.sample(cluster_data, max_per_cluster) if len(cluster_data) > max_per_cluster else cluster_data

reduced_dataset = []
mi_list = []
for cid in clustered_sentences:
    selected = select_uniform(clustered_sentences[cid], max_per_cluster=5)
    reduced_dataset.extend(selected)
    mi_list.append(len(selected))

# خلاصه نتایج
summary = {
    "کل جملات اولیه": len(X_reduced),
    "تعداد خوشه‌ها (k)": k,
    "تعداد جمله‌ها در هر خوشه (mi)": mi_list,
    "کل جملات پس از کاهش": sum(mi_list),
    "بهترین k بر اساس silhouette": best_k,
    "امتیاز silhouette (دستی)": round(best_score, 4),
}
summary

import pandas as pd

df_summary = pd.DataFrame([summary])
# print(df_summary.to_markdown())  # نمایش جدولی در Colab

df_summary

import numpy as np
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
from collections import defaultdict
import random

train_bow = train_bow[:2000]

#  استخراج BOWها از train_bow
labels, vectors = zip(*train_bow)  # train_bow = [(label, bow_vec), ...]
X = np.array(vectors)  # ماتریس ویژگی‌ها

from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(n_components=100, random_state=42)
X_reduced = svd.fit_transform(X)


# KMeans با k=7
k = 2
kmeans = KMeans(n_clusters=k, random_state=42)
clusters = kmeans.fit_predict(X_reduced)

#   خوشه‌بندی فقط روی داده‌های آموزش انجام
#  تست را وارد کنیم، اطلاعات از آینده نشت پیدا می‌کند (data leakage)
# هدف از خوشه‌بندی، تحلیل یا انتخاب داده برای آموزش مدل بعدی است، نه تست.

#  تعیین بهترین k با silhouette score
silhouette_scores = []
k_range = range(2, 11)
for test_k in k_range:
    km = KMeans(n_clusters=test_k, random_state=42).fit(X_reduced)
    score = silhouette_score(X_reduced, km.labels_)
    silhouette_scores.append((test_k, score))

# نمایش silhouette score
best_k, best_score = max(silhouette_scores, key=lambda x: x[1])

#  تجسم خوشه‌ها با PCA
pca = PCA(n_components=2)
X_2d = pca.fit_transform(X_reduced)

plt.figure(figsize=(10, 6))
for cluster_id in range(k):
    plt.scatter(X_2d[clusters == cluster_id, 0],
                X_2d[clusters == cluster_id, 1],
                label=f"Cluster {cluster_id}")
plt.title("پراکنش BOW جملات آموزش با k=7 (نمایش با PCA)")
plt.xlabel("PCA-1")
plt.ylabel("PCA-2")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
X_tsne = tsne.fit_transform(X_reduced)

plt.figure(figsize=(10, 6))
for cluster_id in range(k):
    plt.scatter(X_tsne[clusters == cluster_id, 0],
                X_tsne[clusters == cluster_id, 1],
                label=f"Cluster {cluster_id}")
plt.title("تجسم خوشه‌ها با t-SNE (k=7)")
plt.xlabel("t-SNE-1")
plt.ylabel("t-SNE-2")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


#  کاهش حجم دادگان: انتخاب یکنواخت از هر خوشه
# تعداد m_i برای هر خوشه = min(5, تعداد اعضای خوشه)
clustered_sentences = defaultdict(list)
for i, vec in enumerate(X_reduced):
    clustered_sentences[clusters[i]].append((labels[i], vec))

def select_uniform(cluster_data, max_per_cluster=5):
    if len(cluster_data) <= max_per_cluster:
        return cluster_data
    return random.sample(cluster_data, max_per_cluster)

reduced_dataset = []
mi_list = []
for cid in clustered_sentences:
    selected = select_uniform(clustered_sentences[cid], max_per_cluster=5)
    reduced_dataset.extend(selected)
    mi_list.append(len(selected))

# خروجی نهایی
summary = {
    "کل جملات اولیه": len(X_reduced),
    "تعداد خوشه‌ها (k)": k,
    "تعداد جمله‌ها در هر خوشه (mi)": mi_list,
    "کل جملات پس از کاهش": sum(mi_list),
    "بهترین k بر اساس silhouette": best_k,
    "امتیاز silhouette": round(best_score, 4),
}

summary

ks, scores = zip(*silhouette_scores)
plt.plot(ks, scores, marker='o')
plt.title("Silhouette Score برای مقادیر مختلف k")
plt.xlabel("تعداد خوشه‌ها (k)")
plt.ylabel("Silhouette Score")
plt.grid(True)
plt.show()

# clustered_sentences = defaultdict(list)
# for i, vec in enumerate(X_reduced):
#     clustered_sentences[clusters[i]].append((labels[i], vec))

# def select_uniform(cluster_data, max_per_cluster=5):
#     if len(cluster_data) <= max_per_cluster:
#         return cluster_data
#     return random.sample(cluster_data, max_per_cluster)

# reduced_dataset = []
# mi_list = []
# for cid in clustered_sentences:
#     selected = select_uniform(clustered_sentences[cid], max_per_cluster=5)
#     reduced_dataset.extend(selected)
#     mi_list.append(len(selected))

#----------------------------------------------------------------------------

M_target = len(X_reduced) // 3  # مجموع جمله‌های موردنظر بعد از کاهش حجم

# اندازه هر خوشه
cluster_sizes = {cid: len(clustered_sentences[cid]) for cid in clustered_sentences}
total_size = sum(cluster_sizes.values())

# محاسبه m_i برای هر خوشه
mi_per_cluster = {
    cid: max(1, int(cluster_sizes[cid] * M_target / total_size))  # حداقل 1 از هر خوشه
    for cid in clustered_sentences
}

# انتخاب یکنواخت بر اساس m_i
reduced_dataset = []
for cid, data in clustered_sentences.items():
    mi = mi_per_cluster[cid]
    if len(data) <= mi:
        reduced_dataset.extend(data)
    else:
        reduced_dataset.extend(random.sample(data, mi))

# نتیجه نهایی
M_final = len(reduced_dataset)
print(f" Total reduced sentences selected: {M_final}")
print(f" m_i per cluster: {mi_per_cluster}")

# خروجی نهایی
summary = {
    "کل جملات اولیه": len(X_reduced),
    "تعداد خوشه‌ها (k)": k,
    "تعداد جمله‌ها در هر خوشه (mi)": mi_list,
    "کل جملات پس از کاهش": sum(mi_list),
    "بهترین k بر اساس silhouette": best_k,
    "امتیاز silhouette": round(best_score, 4),
}

"""# دسته بندی

### E₂ ساخت مدل نگاشت
"""

# !pip install tensorflow==2.12

# # ----------------------------
# #  1. آماده‌سازی داده‌های اولیه و پیش‌پردازش
# # ----------------------------
# from torchtext.datasets import IMDB
# import random
# import re
# import nltk
# from nltk.corpus import stopwords
# from collections import Counter
# import numpy as np

# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('popular')
# nltk.download('all')
# stop_words = set(stopwords.words('english'))

#  نسخه بهینه‌شده برای جلوگیری از کرش RAM در آموزش نگاشت E2

# --- مرحله 1: پیش‌پردازش ---
def preprocess(text):
    text = text.lower()
    text = re.sub(r"<.*?>", " ", text)
    text = re.sub(r"[^a-z\s]", " ", text)
    tokens = nltk.word_tokenize(text)
    return [w for w in tokens if w not in stop_words]

train_iter = list(IMDB(split='train'))
train_iter = random.sample(train_iter, len(train_iter) // 2)
train_tokens = [(label, preprocess(text)) for label, text in train_iter]

# حذف واژه‌های نادر
all_tokens = [token for _, tokens in train_tokens for token in tokens]
token_freq = Counter(all_tokens)
def remove_rare_words(tokens, min_freq=2):
    return [token for token in tokens if token_freq[token] >= min_freq]
train_tokens_filtered = [(label, remove_rare_words(tokens)) for label, tokens in train_tokens]

# --- مرحله 2: BOW و خوشه‌بندی ---
vocab = sorted(set(token for _, tokens in train_tokens_filtered for token in tokens))
word2idx = {word: idx for idx, word in enumerate(vocab)}
vocab_size = len(vocab)

def E1(word):
    vec = np.zeros(vocab_size)
    vec[word2idx[word]] = 1
    return vec

def bow_vector(tokens):
    vec = np.zeros(vocab_size)
    for word in tokens:
        if word in word2idx:
            vec += E1(word)
    return vec

full_bow = [(label, bow_vector(tokens), tokens) for label, tokens in train_tokens_filtered]
full_bow = full_bow[:2000]

labels, vectors, token_lists = zip(*full_bow)
X = np.array(vectors)

from sklearn.decomposition import TruncatedSVD
X_reduced = TruncatedSVD(n_components=100, random_state=42).fit_transform(X)

from sklearn.cluster import KMeans
k = 7
clusters = KMeans(n_clusters=k, random_state=42).fit_predict(X_reduced)

# --- مرحله 3: کاهش درصدی داده‌ها ---
from collections import defaultdict
clustered_sentences = defaultdict(list)
for i, vec in enumerate(X_reduced):
    clustered_sentences[clusters[i]].append((labels[i], vec, token_lists[i]))

M_target = 300
cluster_sizes = {cid: len(clustered_sentences[cid]) for cid in clustered_sentences}
total_size = sum(cluster_sizes.values())

mi_per_cluster = {
    cid: max(5, int(cluster_sizes[cid] * M_target / total_size))
    for cid in clustered_sentences
}

reduced_dataset = []
reduced_tokens_filtered = []
for cid, data in clustered_sentences.items():
    mi = mi_per_cluster[cid]
    selected = data if len(data) <= mi else random.sample(data, mi)
    for label, vec, tokens in selected:
        reduced_dataset.append((label, vec))
        reduced_tokens_filtered.append((label, tokens))

# --- مرحله 4: آموزش نگاشت E2 با vocab محدود ---
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical

# محدودسازی واژگان با فراوانی بالا
token_freq = Counter([token for _, tokens in reduced_tokens_filtered for token in tokens])
vocab = sorted(set(token for token in token_freq if token_freq[token] >= 3))
word2idx = {w: i for i, w in enumerate(vocab)}
idx2word = {i: w for w, i in word2idx.items()}
vocab_size = len(vocab)

X_train, y_train = [], []
window = 2
for _, tokens in reduced_tokens_filtered:
    for i, target in enumerate(tokens):
        if target not in word2idx:
            continue
        for j in range(max(0, i - window), min(len(tokens), i + window + 1)):
            if i != j and tokens[j] in word2idx:
                X_train.append(to_categorical(word2idx[target], num_classes=vocab_size))
                y_train.append(to_categorical(word2idx[tokens[j]], num_classes=vocab_size))

X_train = np.array(X_train)
y_train = np.array(y_train)

embedding_dim = 100
model = Sequential([
    Dense(embedding_dim, input_shape=(vocab_size,), activation='linear'),
    Dense(vocab_size, activation='softmax')
])
model.compile(optimizer='adam', loss='categorical_crossentropy')
model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=1)

embedding_matrix = model.layers[0].get_weights()[0]
def E2_custom(word):
    return embedding_matrix[word2idx[word]]

# --- مرحله 5: عملیات برداری ترکیبی ---
def cosine_sim(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

def most_similar_custom(vec):
    return sorted(
        [(w, cosine_sim(vec, E2_custom(w))) for w in vocab],
        key=lambda x: -x[1]
    )[:5]

print(len(train_tokens_filtered))
print(len(reduced_tokens_filtered))

# مثال تمرینی: king - man + woman
if all(w in word2idx for w in ["king", "man", "woman"]):
    vec_query = E2_custom("king") - E2_custom("man") + E2_custom("woman")
    for word, score in most_similar_custom(vec_query):
        print(f"{word}: {score:.4f}")
else:
    print("کلمات مورد نظر در واژه‌نامه موجود نیستند.")

# مثال تمرینی: great - boring + recommend
if all(w in word2idx for w in ["great", "boring", "recommend"]):
    vec_query = E2_custom("great") - E2_custom("boring") + E2_custom("recommend")
    for word, score in most_similar_custom(vec_query):
        print(f"{word}: {score:.4f}")
else:
    print("کلمات مورد نظر در واژه‌نامه موجود نیستند.")

# # !pip install --upgrade numpy gensim

# !pip install gensim==4.3.1 scipy==1.10.1 --upgrade --force-reinstall

import gensim.downloader as api
wv = api.load('word2vec-google-news-300')

vec_google = wv["great"] - wv["boring"] + wv["recommend"]
similar_words = wv.similar_by_vector(vec_google, topn=5)

print("\n مدل آماده word2vec (Google News):")
for word, sim in similar_words:
    print(f"{word}: {sim:.4f}")

"""# جمله ← بردار میانگین"""

# # 1. ساخت توالی بردارها یا میانگین بردار جمله
# import numpy as np
# from tensorflow.keras.preprocessing.sequence import pad_sequences

# # فرض: reduced_tokens_filtered = [(label, [tokens])]
# # فرض: E2_custom(word) در دسترس است و کار می‌کند

# MAX_LEN = 50  # حداکثر طول جمله

# def sentence_vector_sequence(tokens):
#     vecs = [E2_custom(w) for w in tokens if w in word2idx]
#     return vecs

# def sentence_vector_average(tokens):
#     vecs = [E2_custom(w) for w in tokens if w in word2idx]
#     if vecs:
#         return np.mean(vecs, axis=0)
#     else:
#         return np.zeros(100)

# # داده‌ها برای مدل RNN (sequence-based)
# X_seq = [sentence_vector_sequence(tokens) for _, tokens in reduced_tokens_filtered]
# X_seq_padded = pad_sequences(X_seq, maxlen=MAX_LEN, dtype='float32', padding='post', truncating='post')

# # داده‌ها برای مدل CNN (همین توالی‌ها مناسبند)
# # داده‌ها برای مدل MLP (میانگین بردار جمله‌ها)
# X_avg = np.array([sentence_vector_average(tokens) for _, tokens in reduced_tokens_filtered])
# y = np.array([int(label) for label, _ in reduced_tokens_filtered])

"""# Final

### دسته بندی
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import numpy as np

EMBED_DIM = 300  # اگر از wv استفاده شود
MAX_LEN = 100

X_seq = []
y_seq = []

for label, tokens in reduced_tokens_filtered:
    seq = []
    for word in tokens:
        if word in wv:
            seq.append(wv[word])
    if len(seq) > 0:
        X_seq.append(seq)
        y_seq.append(1 if label in ['pos', '2'] else 0)

X_seq_padded = pad_sequences(X_seq, maxlen=MAX_LEN, dtype='float32', padding='post', truncating='post')
y_seq = np.array(y_seq)

# --- تقسیم train/test
X_train, X_test, y_train, y_test = train_test_split(X_seq_padded, y_seq, test_size=0.3, random_state=42)

# -------------------------
#  مدل RNN
# -------------------------
model_rnn = Sequential([
    LSTM(64, return_sequences=False, input_shape=(MAX_LEN, EMBED_DIM)),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])
model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_rnn.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# -------------------------
#  مدل CNN
# -------------------------
model_cnn = Sequential([
    Conv1D(128, 5, activation='relu', input_shape=(MAX_LEN, EMBED_DIM)),
    GlobalMaxPooling1D(),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])
model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_cnn.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# -------------------------
#  مدل ترکیبی CNN + RNN
# -------------------------
model_hybrid = Sequential([
    Conv1D(64, 5, activation='relu', input_shape=(MAX_LEN, EMBED_DIM)),
    Dropout(0.3),
    LSTM(64),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])
model_hybrid.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_hybrid.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# --- ارزیابی نهایی

print("\n ارزیابی مدل‌ها:")
print(" دقت RNN:", model_rnn.evaluate(X_test, y_test, verbose=0)[1])
print(" دقت CNN:", model_cnn.evaluate(X_test, y_test, verbose=0)[1])
print(" دقت ترکیبی:", model_hybrid.evaluate(X_test, y_test, verbose=0)[1])

import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
# import gensim.downloader as api

# بارگذاری word2vec گوگل
# wv = api.load("word2vec-google-news-300")
EMBED_DIM = 300

# فرض: reduced_tokens_filtered = [(label, [tokens])]
# نگاشت لیبل 1→0 و 2→1
def sentence_vector_average(tokens):
    vecs = [wv[word] for word in tokens if word in wv]
    return np.mean(vecs, axis=0) if vecs else np.zeros(EMBED_DIM)

X_avg = np.array([sentence_vector_average(tokens) for label, tokens in reduced_tokens_filtered])
y_avg = np.array([0 if label == 1 else 1 for label, _ in reduced_tokens_filtered])

# تقسیم داده‌ها
X_train, X_test, y_train, y_test = train_test_split(X_avg, y_avg, test_size=0.2, random_state=42)

# مدل MLP
model = Sequential([
    Dense(128, activation='relu', input_shape=(EMBED_DIM,)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=20, batch_size=16, validation_data=(X_test, y_test))

# ارزیابی
loss, acc = model.evaluate(X_test, y_test)
print(" دقت MLP با Word2Vec میانگین:", acc)

import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Conv1D, GlobalMaxPooling1D, Dense, Dropout, MaxPooling1D
from tensorflow.keras.preprocessing.sequence import pad_sequences
# import gensim.downloader as api

# بارگذاری word2vec آماده گوگل
# wv = api.load("word2vec-google-news-300")
EMBED_DIM = 300
MAX_LEN = 50

# ساخت توالی برداری
def sentence_vector_sequence(tokens):
    return [wv[word] for word in tokens if word in wv]

# فرض: reduced_tokens_filtered = [(label, tokens)]
X_seq = [sentence_vector_sequence(tokens) for _, tokens in reduced_tokens_filtered]
y = np.array([0 if label == 1 else 1 for label, _ in reduced_tokens_filtered])

# پدینگ توالی‌ها
X_seq_padded = pad_sequences(X_seq, maxlen=MAX_LEN, dtype='float32', padding='post', truncating='post')

# تقسیم داده‌ها
X_train, X_test, y_train, y_test = train_test_split(X_seq_padded, y, test_size=0.2, random_state=42)

# -------------------------------
# مدل RNN
# -------------------------------
model_rnn = Sequential([
    LSTM(64, input_shape=(MAX_LEN, EMBED_DIM), dropout=0.2, recurrent_dropout=0.2),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])
model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_rnn.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))
print(" دقت RNN:", model_rnn.evaluate(X_test, y_test, verbose=0)[1])

# -------------------------------
# مدل CNN
# -------------------------------
model_cnn = Sequential([
    Conv1D(128, 5, activation='relu', input_shape=(MAX_LEN, EMBED_DIM)),
    GlobalMaxPooling1D(),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])
model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_cnn.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))
print(" دقت CNN:", model_cnn.evaluate(X_test, y_test, verbose=0)[1])

# -------------------------------
# مدل ترکیبی CNN + LSTM
# -------------------------------
model_hybrid = Sequential([
    Conv1D(64, 5, activation='relu', input_shape=(MAX_LEN, EMBED_DIM)),
    MaxPooling1D(pool_size=2),
    Dropout(0.2),
    LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])
model_hybrid.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_hybrid.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))
print(" دقت ترکیبی:", model_hybrid.evaluate(X_test, y_test, verbose=0)[1])

print("**************************************************************")
# ارزیابی نهایی
print("\n دقت‌ها:")
print(" دقت RNN:", model_rnn.evaluate(X_test, y_test, verbose=0)[1])
print(" دقت CNN:", model_cnn.evaluate(X_test, y_test, verbose=0)[1])
print(" دقت ترکیبی:", model_hybrid.evaluate(X_test, y_test, verbose=0)[1])

import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, GlobalMaxPooling1D
from tensorflow.keras.preprocessing.sequence import pad_sequences

# پارامترها
MAX_LEN = 50
EMBED_DIM = 100

# فرض: reduced_tokens_filtered = [(label, [tokens])]
# فرض: E2_custom(word) و word2idx از قبل تعریف شده‌اند

# ساخت توالی بردار برای RNN/CNN
def sentence_vector_sequence(tokens):
    vecs = [E2_custom(w) for w in tokens if w in word2idx]
    return vecs

# ساخت میانگین بردار برای MLP
def sentence_vector_average(tokens):
    vecs = [E2_custom(w) for w in tokens if w in word2idx]
    return np.mean(vecs, axis=0) if vecs else np.zeros(EMBED_DIM)

# آماده‌سازی داده‌ها
X_seq = [sentence_vector_sequence(tokens) for _, tokens in reduced_tokens_filtered]
X_seq_padded = pad_sequences(X_seq, maxlen=MAX_LEN, dtype='float32', padding='post', truncating='post')
X_avg = np.array([sentence_vector_average(tokens) for _, tokens in reduced_tokens_filtered])
y = np.array([int(label) for label, _ in reduced_tokens_filtered])

# تقسیم آموزش/آزمون
X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(X_seq_padded, y, test_size=0.2, random_state=42)
X_train_avg, X_test_avg, y_train_avg, y_test_avg = train_test_split(X_avg, y, test_size=0.2, random_state=42)

# مدل MLP
model_mlp = Sequential([
    Dense(64, activation='relu', input_shape=(EMBED_DIM,)),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
model_mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_mlp.fit(X_train_avg, y_train_avg, epochs=10, batch_size=16, validation_data=(X_test_avg, y_test_avg))

# مدل RNN
model_rnn = Sequential([
    LSTM(64, input_shape=(MAX_LEN, EMBED_DIM), dropout=0.4, recurrent_dropout=0.4),
    Dense(32, activation='relu'),
    Dropout(0.4),
    Dense(1, activation='sigmoid')
])
model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_rnn.fit(X_train_seq, y_train_seq, epochs=10, batch_size=16, validation_data=(X_test_seq, y_test_seq))

# مدل CNN
model_cnn = Sequential([
    Conv1D(128, 5, activation='relu', input_shape=(MAX_LEN, EMBED_DIM)),
    GlobalMaxPooling1D(),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_cnn.fit(X_train_seq, y_train_seq, epochs=10, batch_size=16, validation_data=(X_test_seq, y_test_seq))

# مدل ترکیبی RNN + CNN
model_hybrid = Sequential([
    Conv1D(64, 5, activation='relu', input_shape=(MAX_LEN, EMBED_DIM)),
    Dropout(0.3),
    LSTM(64, dropout=0.3, recurrent_dropout=0.3),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])
model_hybrid.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_hybrid.fit(X_train_seq, y_train_seq, epochs=10, batch_size=16, validation_data=(X_test_seq, y_test_seq))

# ارزیابی نهایی
print("\n دقت‌ها:")
print(" دقت MLP:", model_mlp.evaluate(X_test_avg, y_test_avg, verbose=0)[1])
print(" دقت RNN:", model_rnn.evaluate(X_test_seq, y_test_seq, verbose=0)[1])
print(" دقت CNN:", model_cnn.evaluate(X_test_seq, y_test_seq, verbose=0)[1])
print(" دقت ترکیبی:", model_hybrid.evaluate(X_test_seq, y_test_seq, verbose=0)[1])

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, GlobalMaxPooling1D
from tensorflow.keras.preprocessing.sequence import pad_sequences

# فرض: reduced_tokens_filtered = [(label, [tokens])]
# فرض: E2_custom(word) و word2idx از قبل تعریف شده‌اند

MAX_LEN = 50
EMBED_DIM = 100

# --- ساخت توالی و میانگین بردار ---
def sentence_vector_sequence(tokens):
    return [E2_custom(w) for w in tokens if w in word2idx]

def sentence_vector_average(tokens):
    vecs = [E2_custom(w) for w in tokens if w in word2idx]
    return np.mean(vecs, axis=0) if vecs else np.zeros(EMBED_DIM)

# --- آماده‌سازی داده‌ها ---
X_seq = [sentence_vector_sequence(tokens) for _, tokens in reduced_tokens_filtered]
X_seq_padded = pad_sequences(X_seq, maxlen=MAX_LEN, dtype='float32', padding='post', truncating='post')
X_avg = np.array([sentence_vector_average(tokens) for _, tokens in reduced_tokens_filtered])

#  تبدیل labelهای 1→0 و 2→1
y = np.array([0 if label == 1 else 1 for label, _ in reduced_tokens_filtered])

# --- تقسیم داده ---
X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(X_seq_padded, y, test_size=0.2, random_state=42)
X_train_avg, X_test_avg, y_train_avg, y_test_avg = train_test_split(X_avg, y, test_size=0.2, random_state=42)

# --- محاسبه وزن کلاس‌ها برای مقابله با عدم‌تعادل ---
cw_array = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)
cw = {0: cw_array[0], 1: cw_array[1]}

# -------------------------
#  مدل MLP
# -------------------------
model_mlp = Sequential([
    Dense(64, activation='relu', input_shape=(EMBED_DIM,)),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])
model_mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_mlp.fit(X_train_avg, y_train_avg, epochs=20, batch_size=16, validation_data=(X_test_avg, y_test_avg), class_weight=cw)
print("=============================================")
# -------------------------
#  مدل RNN
# -------------------------
model_rnn = Sequential([
    LSTM(64, input_shape=(MAX_LEN, EMBED_DIM), dropout=0.2, recurrent_dropout=0.2),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])
model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_rnn.fit(X_train_seq, y_train_seq, epochs=20, batch_size=32, validation_data=(X_test_seq, y_test_seq), class_weight=cw)
print("=============================================")
# -------------------------
#  مدل CNN
# -------------------------
model_cnn = Sequential([
    Conv1D(128, 5, activation='relu', input_shape=(MAX_LEN, EMBED_DIM)),
    GlobalMaxPooling1D(),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])
model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_cnn.fit(X_train_seq, y_train_seq, epochs=20, batch_size=32, validation_data=(X_test_seq, y_test_seq), class_weight=cw)
print("=============================================")
# -------------------------
#  مدل ترکیبی Hybrid (CNN + LSTM)
# -------------------------
from tensorflow.keras.layers import MaxPooling1D

model_hybrid = Sequential([
    Conv1D(64, 5, activation='relu', input_shape=(MAX_LEN, EMBED_DIM)),
    MaxPooling1D(pool_size=2),
    Dropout(0.2),
    LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])
model_hybrid.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_hybrid.fit(X_train_seq, y_train_seq, epochs=30, batch_size=32, validation_data=(X_test_seq, y_test_seq), class_weight=cw)
print("=============================================")
# -------------------------
#  ارزیابی نهایی
# -------------------------
print("\n دقت‌ها:")
print(" دقت MLP:", model_mlp.evaluate(X_test_avg, y_test_avg, verbose=0)[1])
print(" دقت RNN:", model_rnn.evaluate(X_test_seq, y_test_seq, verbose=0)[1])
print(" دقت CNN:", model_cnn.evaluate(X_test_seq, y_test_seq, verbose=0)[1])
print(" دقت ترکیبی:", model_hybrid.evaluate(X_test_seq, y_test_seq, verbose=0)[1])