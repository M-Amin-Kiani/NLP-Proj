{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# الگوریتم مقایسه شباهت (Levenshtein/SequenceMatcher)  20%"
      ],
      "metadata": {
        "id": "YULO3240Nmbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhKkbTBEfRDG",
        "outputId": "ba7028b1-6884-4ec4-bd77-2bd5d5b95cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original sentence:\n",
            "The quick brown fox jumps over the lazy dog.\n",
            "\n",
            "Misspelled sentence:\n",
            "Th quik braown foxs jmups ovem te lagzy qog.\n",
            "\n",
            "Corrected sentence:\n",
            "The quirk brown foxes mumps ovem te lazy qog.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from difflib import get_close_matches\n",
        "\n",
        "# مرحله ۱: لود کردن دیکشنری لغات از فایل\n",
        "with open(\"/content/Vocabulary.txt\", \"r\") as f:\n",
        "    vocabulary = set(word.strip().lower() for word in f if word.strip())\n",
        "\n",
        "# مرحله ۲: جمله‌ی دارای غلط املایی\n",
        "# The quick brown fox jumps over the lazy dog.\n",
        "misspelled_sentence = \"Th quik braown foxs jmups ovem te lagzy qog.\"\n",
        "\n",
        "# مرحله ۳: توکنایز جمله به کلمات\n",
        "words = re.findall(r\"\\b\\w+\\b\", misspelled_sentence.lower())\n",
        "\n",
        "# تابع اصلاح غلط‌های املایی\n",
        "def correct_word(word, vocab):\n",
        "    matches = get_close_matches(word, vocab, n=1, cutoff=0.8)\n",
        "    return matches[0] if matches else word\n",
        "\n",
        "# اصلاح تمام کلمات جمله\n",
        "corrected_words = [correct_word(word, vocabulary) for word in words]\n",
        "\n",
        "# بازسازی جمله اصلاح‌شده\n",
        "corrected_sentence = \" \".join(corrected_words).capitalize() + \".\"\n",
        "\n",
        "# نمایش نتیجه‌ها\n",
        "print(\"\\nOriginal sentence:\")\n",
        "print(\"The quick brown fox jumps over the lazy dog.\")\n",
        "print(\"\\nMisspelled sentence:\")\n",
        "print(misspelled_sentence)\n",
        "print(\"\\nCorrected sentence:\")\n",
        "print(corrected_sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib\n",
        "import re\n",
        "\n",
        "# Load the vocabulary from the uploaded file\n",
        "with open(\"/content/Vocabulary.txt\", \"r\") as file:\n",
        "    vocabulary = set(word.strip().lower() for word in file.readlines() if word.strip())\n",
        "\n",
        "# Input: Misspelled sentence\n",
        "misspelled_sentence = \"Th quik braown foxs jmups ovem te lagzy qog.\"\n",
        "\n",
        "# Improved tokenizer (handles punctuation better)\n",
        "def tokenize(sentence):\n",
        "    return re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", sentence)\n",
        "\n",
        "# Function to find the most similar word using difflib with a higher cutoff\n",
        "def correct_word(word, vocab, cutoff=0.85):\n",
        "    word = word.lower()\n",
        "    matches = difflib.get_close_matches(word, vocab, n=1, cutoff=cutoff)\n",
        "    return matches[0] if matches else word\n",
        "\n",
        "# Correct the sentence\n",
        "def correct_sentence(sentence, vocab):\n",
        "    tokens = tokenize(sentence)\n",
        "    corrected_tokens = []\n",
        "    for token in tokens:\n",
        "        if re.match(r\"\\w+\", token):  # If it's a word\n",
        "            corrected = correct_word(token, vocab)\n",
        "            corrected_tokens.append(corrected)\n",
        "        else:\n",
        "            corrected_tokens.append(token)  # Keep punctuation as is\n",
        "    return ' '.join(corrected_tokens).replace(\" .\", \".\")\n",
        "\n",
        "corrected_sentence = correct_sentence(misspelled_sentence, vocabulary)\n",
        "corrected_sentence\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "88JPpNK0OLBa",
        "outputId": "03a65a10-f4a1-4654-e7cc-fff32e6e7b85"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'th quirk brown foxes jmups ovem te lazy qog.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# الگوریتم اصلاح‌کننده‌ی نورویگ (Norvig Spell Corrector) 20%\n",
        "###فقط بر اساس شباهت نگارشی و احتمال آماری عمل می‌کنه، بدون درک معنی جمله"
      ],
      "metadata": {
        "id": "c_jgrP-3R_9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "# Load vocabulary again to ensure clean state\n",
        "with open(\"/content/Vocabulary.txt\", \"r\") as f:\n",
        "    vocabulary = set(word.strip().lower() for word in f.readlines() if word.strip())\n",
        "\n",
        "# Create word frequency using vocabulary (simulate frequency)\n",
        "word_freq = Counter({word: 1 for word in vocabulary})\n",
        "\n",
        "# Norvig’s original probability-based spell corrector\n",
        "\n",
        "def words(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "def P(word, N=sum(word_freq.values())):\n",
        "    return word_freq[word] / N\n",
        "\n",
        "def correction(word):\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word):\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words_set):\n",
        "    return set(w for w in words_set if w in vocabulary)\n",
        "\n",
        "def edits1(word):\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [L + R[1:] for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "    inserts = [L + c + R for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "# Apply Norvig-based corrector on sentence\n",
        "def norvig_correct_sentence(sentence):\n",
        "    tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", sentence)\n",
        "    corrected = []\n",
        "    for token in tokens:\n",
        "        if token.isalpha():\n",
        "            corrected.append(correction(token.lower()))\n",
        "        else:\n",
        "            corrected.append(token)\n",
        "    return ' '.join(corrected).replace(\" .\", \".\")\n",
        "\n",
        "# Test sentence with errors\n",
        "test_sentence = \"Th quik braown foxs jmups ovem te lagzy qog.\"\n",
        "norvig_result = norvig_correct_sentence(test_sentence)\n",
        "norvig_result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yeqKRDeyQhO1",
        "outputId": "ce0f7d7f-27a6-4eef-c23d-93bdc0d0ff95"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tb quip brawn foxy jumps ovum te lazy tog.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SymSpell 40% الگوریتم -"
      ],
      "metadata": {
        "id": "fq5y1PwFTgyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install symspellpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFR2vaJMTRfJ",
        "outputId": "78146894-e5ed-4556-d367-a0354db9d7f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting symspellpy\n",
            "  Downloading symspellpy-6.9.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting editdistpy>=0.1.3 (from symspellpy)\n",
            "  Downloading editdistpy-0.1.5-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Downloading symspellpy-6.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading editdistpy-0.1.5-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.1/144.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: editdistpy, symspellpy\n",
            "Successfully installed editdistpy-0.1.5 symspellpy-6.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "import re\n",
        "\n",
        "# Initialize SymSpell\n",
        "max_edit_distance_dictionary = 2\n",
        "prefix_length = 7\n",
        "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
        "\n",
        "# Load dictionary\n",
        "sym_spell.load_dictionary(\"/content/Vocabulary.txt\", term_index=0, count_index=1)\n",
        "\n",
        "# تصحیح جمله\n",
        "def symspell_correct_sentence(sentence):\n",
        "    tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", sentence)\n",
        "    corrected = []\n",
        "    for token in tokens:\n",
        "        if token.isalpha():\n",
        "            suggestions = sym_spell.lookup(token.lower(), Verbosity.CLOSEST, max_edit_distance=2)\n",
        "            corrected_word = suggestions[0].term if suggestions else token\n",
        "            corrected.append(corrected_word)\n",
        "        else:\n",
        "            corrected.append(token)\n",
        "    return ' '.join(corrected).replace(\" .\", \".\")\n",
        "\n",
        "# تست\n",
        "test_sentence = \"Th quik braown foxs jmups ovem te lagzy qog.\"\n",
        "print(symspell_correct_sentence(test_sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byRnTmrrS4lm",
        "outputId": "995bfcdc-789f-40cc-d550-a2ce73d83b58"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the quirk brown foxes jumps ovum te lazy log.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI API (پولی - آنلاین) 80-100%"
      ],
      "metadata": {
        "id": "eOo51X_DWAV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = \"My_API_KEY\"\n",
        "\n",
        "def gpt_correct_sentence(sentence):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a grammar correction assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Please correct this sentence: {sentence}\"}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "# تست\n",
        "test_sentence = \"Th quik braown foxs jmups ovem te lagzy qog.\"\n",
        "print(gpt_correct_sentence(test_sentence))\n"
      ],
      "metadata": {
        "id": "44T7uiQDVOzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT + Gramformer (آفلاین)   90%"
      ],
      "metadata": {
        "id": "paT0NRO9WWRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gramformer\n",
        "!pip install torch transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZk_qehwWV94",
        "outputId": "e0d4a62b-d2ec-40c9-8d69-f7fbe2445fcc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement gramformer (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for gramformer\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install language-tool-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHRQGa8JX3lf",
        "outputId": "4d69de8f-a3ee-474e-8d92-8c7cd71c27cf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.9.0-py3-none-any.whl.metadata (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (5.9.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from language-tool-python) (0.10.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->language-tool-python) (2025.1.31)\n",
            "Downloading language_tool_python-2.9.0-py3-none-any.whl (49 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from gramformer import Gramformer\n",
        "\n",
        "# gf = Gramformer(models=1)  # 1 = Corrector\n",
        "\n",
        "# def bert_correct_sentence(sentence):\n",
        "#     corrected = list(gf.correct(sentence, max_candidates=1))\n",
        "#     return corrected[0] if corrected else sentence\n",
        "\n",
        "# # تست\n",
        "# print(bert_correct_sentence(\"Th quik braown foxs jmups ovem te lagzy qog.\"))\n",
        "\n",
        "import language_tool_python\n",
        "\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "\n",
        "text = \"Th quik braown foxs jmups ovem te lagzy qog.\"\n",
        "matches = tool.check(text)\n",
        "corrected_text = language_tool_python.utils.correct(text, matches)\n",
        "\n",
        "print(corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh04EmWmWs9E",
        "outputId": "de2fc0c3-262c-4c75-d749-48e30252f894"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading LanguageTool 6.5: 100%|██████████| 248M/248M [00:03<00:00, 68.9MB/s]\n",
            "INFO:language_tool_python.download_lt:Unzipping /tmp/tmpbvxv54mh.zip to /root/.cache/language_tool_python.\n",
            "INFO:language_tool_python.download_lt:Downloaded https://www.languagetool.org/download/LanguageTool-6.5.zip to /root/.cache/language_tool_python.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown foxes jumps over the lazy dog.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Edit Distance + Context Score %الگوریتم  60"
      ],
      "metadata": {
        "id": "hFN1Bo23Yu7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ترکیب فاصله ویرایشی و بررسی موقعیت کلمه در جمله\n",
        "\n",
        "### کلمات درست‌تر با توجه به جایگاهشون در جمله انتخاب"
      ],
      "metadata": {
        "id": "XgyCgRQ2ZoHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# Re-load Vocabulary file\n",
        "vocab_path = \"/content/Vocabulary.txt\"\n",
        "with open(vocab_path, \"r\") as file:\n",
        "    vocabulary = set(word.strip().lower() for word in file.readlines() if word.strip())\n",
        "\n",
        "# Corrupted sentence to test\n",
        "sentence = \"Th quik braown foxs jmups ovem te lagzy qog.\"\n",
        "\n",
        "# Function to calculate similarity ratio\n",
        "def similarity_ratio(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "# Get best match from vocabulary\n",
        "def get_best_match(word, vocab, threshold=0.7):\n",
        "    suggestions = [(v, similarity_ratio(word, v)) for v in vocab]\n",
        "    suggestions = [s for s in suggestions if s[1] >= threshold]\n",
        "    suggestions.sort(key=lambda x: -x[1])  # Sort by descending similarity\n",
        "    return suggestions[0][0] if suggestions else word\n",
        "\n",
        "# Sentence correction using edit distance + context scoring\n",
        "def context_based_correction(sentence, vocab):\n",
        "    tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", sentence)\n",
        "    corrected_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.isalpha():\n",
        "            best = get_best_match(token.lower(), vocab)\n",
        "            corrected_tokens.append(best)\n",
        "        else:\n",
        "            corrected_tokens.append(token)\n",
        "    return ' '.join(corrected_tokens).replace(\" .\", \".\")\n",
        "\n",
        "# Apply the algorithm\n",
        "corrected_sentence = context_based_correction(sentence, vocabulary)\n",
        "corrected_sentence\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HwSLgbB9YBZA",
        "outputId": "3b98cb5e-a825-442b-f153-c7b66e95a9cc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'th quirk 1 brown 1 foxes 1 jmups ovem te lazy 1 qog.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Weighted Scoring الگوریتم"
      ],
      "metadata": {
        "id": "AfhTCSmpae7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ترکیبی از:\n",
        "\n",
        "###شباهت نگارشی (edit distance) - طول کلمه - شروع و پایان مشابه - تکرار حروف"
      ],
      "metadata": {
        "id": "FQxeCpVEah9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load and clean vocabulary (ASCII only)\n",
        "vocab_path = \"/content/Vocabulary.txt\"\n",
        "with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    vocabulary = set(word.strip().lower() for word in file if word.strip() and word.strip().isascii())\n",
        "\n",
        "# --- COMPONENT 1: Edit Distance similarity ---\n",
        "def similarity_ratio(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def get_best_match_edit(word, vocab, threshold=0.7):\n",
        "    suggestions = [(v, similarity_ratio(word, v)) for v in vocab]\n",
        "    suggestions = [s for s in suggestions if s[1] >= threshold]\n",
        "    suggestions.sort(key=lambda x: -x[1])\n",
        "    return suggestions[0][0] if suggestions else word\n",
        "\n",
        "# --- COMPONENT 2: Phonetic Matching (Soundex) ---\n",
        "def soundex(word):\n",
        "    word = word.upper()\n",
        "    if not word or not word.isascii():\n",
        "        return \"\"\n",
        "    replacements = {\n",
        "        \"BFPV\": '1', \"CGJKQSXZ\": '2', \"DT\": '3',\n",
        "        \"L\": '4', \"MN\": '5', \"R\": '6'\n",
        "    }\n",
        "    result = word[0]\n",
        "    mapping = {char: code for group, code in replacements.items() for char in group}\n",
        "    for char in word[1:]:\n",
        "        code = mapping.get(char, '')\n",
        "        if code != result[-1]:\n",
        "            result += code\n",
        "    result = re.sub(r'[^A-Z0-9]', '', result)\n",
        "    return (result + '000')[:4]\n",
        "\n",
        "def get_best_match_phonetic(word, vocab):\n",
        "    token_sound = soundex(word)\n",
        "    matches = [v for v in vocab if soundex(v) == token_sound]\n",
        "    return matches[0] if matches else word\n",
        "\n",
        "# --- FINAL HYBRID CORRECTION FUNCTION ---\n",
        "def hybrid_correction(sentence, vocab, edit_threshold=0.7):\n",
        "    tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", sentence)\n",
        "    corrected_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token.isalpha():\n",
        "            token_lower = token.lower()\n",
        "\n",
        "            # Try Edit Distance first\n",
        "            edit_match = get_best_match_edit(token_lower, vocab, threshold=edit_threshold)\n",
        "\n",
        "            # If edit match is too far, try phonetic\n",
        "            if edit_match == token_lower:\n",
        "                phonetic_match = get_best_match_phonetic(token_lower, vocab)\n",
        "                corrected_tokens.append(phonetic_match)\n",
        "            else:\n",
        "                corrected_tokens.append(edit_match)\n",
        "        else:\n",
        "            corrected_tokens.append(token)\n",
        "\n",
        "    return ' '.join(corrected_tokens).replace(\" .\", \".\")\n",
        "\n",
        "# Test on sample input\n",
        "test_sentence = \"Th quik braown foxs jmups ovem te lagzy qog.\"\n",
        "hybrid_result = hybrid_correction(test_sentence, vocabulary)\n",
        "hybrid_result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Myf5_ZOYaehz",
        "outputId": "70f4c0f2-9e4c-4c56-cd6f-4a6d9eef3ade"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tow 1 quirk 1 brown 1 foxes 1 jumps 1 oven 1 tow 1 lazy 1 quack 1.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phonetic Matching (Soundex یا Metaphone) الگوریتم %10\n",
        "\n",
        "###تصحیح بر اساس صدا، نه صرفاً نوشتار. مثل وقتی می‌نویسی fone به‌جای phone.\n",
        "\n"
      ],
      "metadata": {
        "id": "i63I1k87bGMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtanKHv7bORA",
        "outputId": "d31eb566-6ebd-418f-b778-e814361d21d6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzy\n",
            "  Downloading Fuzzy-1.2.2.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fuzzy\n",
            "  Building wheel for fuzzy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuzzy: filename=Fuzzy-1.2.2-cp311-cp311-linux_x86_64.whl size=220703 sha256=97c598d1997673d7565fa3e8ac29bdba1348b8986a120460ed3373fee53d4850\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/1c/77/28af87176ebf6eb6208c17e64a45a8e48eda4194bd8f605096\n",
            "Successfully built fuzzy\n",
            "Installing collected packages: fuzzy\n",
            "Successfully installed fuzzy-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# Reload vocabulary file\n",
        "vocab_path = \"/content/Vocabulary.txt\"\n",
        "with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    vocabulary = set(word.strip().lower() for word in file.readlines() if word.strip() and word.strip().isascii())\n",
        "\n",
        "# Improved phonetic correction using Soundex with ASCII-safe fallback\n",
        "def soundex(word):\n",
        "    word = word.upper()\n",
        "    if not word.isascii() or not word:\n",
        "        return \"\"\n",
        "    replacements = {\n",
        "        \"BFPV\": '1',\n",
        "        \"CGJKQSXZ\": '2',\n",
        "        \"DT\": '3',\n",
        "        \"L\": '4',\n",
        "        \"MN\": '5',\n",
        "        \"R\": '6'\n",
        "    }\n",
        "    result = word[0]\n",
        "    mapping = {char: code for group, code in replacements.items() for char in group}\n",
        "\n",
        "    for char in word[1:]:\n",
        "        code = mapping.get(char, '')\n",
        "        if code != result[-1]:\n",
        "            result += code\n",
        "\n",
        "    result = re.sub(r'[^A-Z0-9]', '', result)  # Remove non-ASCII\n",
        "    return (result + '000')[:4]\n",
        "\n",
        "# Apply phonetic correction using our Soundex\n",
        "def phonetic_correction(sentence, vocab):\n",
        "    tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", sentence)\n",
        "    corrected = []\n",
        "    for token in tokens:\n",
        "        if token.isalpha():\n",
        "            token_sound = soundex(token)\n",
        "            matches = [v for v in vocab if soundex(v) == token_sound]\n",
        "            best = matches[0] if matches else token\n",
        "            corrected.append(best)\n",
        "        else:\n",
        "            corrected.append(token)\n",
        "    return ' '.join(corrected).replace(\" .\", \".\")\n",
        "\n",
        "# Run the correction on the test sentence\n",
        "test_sentence = \"Th quik braown foxs jmups ovem te lagzy qog.\"\n",
        "phonetic_result = phonetic_correction(test_sentence, vocabulary)\n",
        "phonetic_result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1Hm1WEezbZOj",
        "outputId": "fb6ed07c-0092-4869-a6b1-fa4c9b725b74"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tow 1 quack 1 bernie 1 fuzzy 1 jumps 1 oven 1 tow 1 lucks 1 quack 1.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed vocabulary loading: use only the first word in each line (ignore any frequency or trailing data)\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "from collections import Counter\n",
        "\n",
        "# Properly load only the first word from each line\n",
        "vocab_path = \"/content/Vocabulary.txt\"\n",
        "with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    vocab_words = [line.strip().split()[0].lower() for line in file if line.strip() and line.strip()[0].isascii()]\n",
        "    vocabulary = set(vocab_words)\n",
        "    word_freq = Counter(vocab_words)\n",
        "\n",
        "# Component 1: Edit Distance + Probability\n",
        "def similarity_ratio(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def P(word):\n",
        "    N = sum(word_freq.values())\n",
        "    return word_freq[word] / N if word in word_freq else 1 / N\n",
        "\n",
        "def get_best_probable_match(word, vocab, threshold=0.5):\n",
        "    suggestions = [(v, similarity_ratio(word, v), P(v)) for v in vocab if similarity_ratio(word, v) >= threshold]\n",
        "    if not suggestions:\n",
        "        return word\n",
        "    suggestions.sort(key=lambda x: (-x[1], -x[2]))\n",
        "    return suggestions[0][0]\n",
        "\n",
        "# Component 2: Soundex phonetic match\n",
        "def soundex(word):\n",
        "    word = word.upper()\n",
        "    if not word or not word.isascii():\n",
        "        return \"\"\n",
        "    replacements = {\n",
        "        \"BFPV\": '1', \"CGJKQSXZ\": '2', \"DT\": '3',\n",
        "        \"L\": '4', \"MN\": '5', \"R\": '6'\n",
        "    }\n",
        "    result = word[0]\n",
        "    mapping = {char: code for group, code in replacements.items() for char in group}\n",
        "    for char in word[1:]:\n",
        "        code = mapping.get(char, '')\n",
        "        if code != result[-1]:\n",
        "            result += code\n",
        "    return (result + '000')[:4]\n",
        "\n",
        "def get_best_match_phonetic(word, vocab):\n",
        "    token_sound = soundex(word)\n",
        "    matches = [v for v in vocab if soundex(v) == token_sound]\n",
        "    return matches[0] if matches else word\n",
        "\n",
        "# Fallback: Edit Distance Only\n",
        "def get_best_match_edit(word, vocab, threshold=0.7):\n",
        "    suggestions = [(v, similarity_ratio(word, v)) for v in vocab if similarity_ratio(word, v) >= threshold]\n",
        "    suggestions.sort(key=lambda x: -x[1])\n",
        "    return suggestions[0][0] if suggestions else word\n",
        "\n",
        "# Final hybrid correction\n",
        "def hybrid_correction_clean(sentence, vocab, edit_threshold=0.7):\n",
        "    tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", sentence)\n",
        "    corrected_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token.isalpha():\n",
        "            token_lower = token.lower()\n",
        "\n",
        "            best = get_best_probable_match(token_lower, vocab, threshold=edit_threshold)\n",
        "            if best == token_lower:\n",
        "                best = get_best_match_phonetic(token_lower, vocab)\n",
        "            if best == token_lower:\n",
        "                best = get_best_match_edit(token_lower, vocab, threshold=edit_threshold)\n",
        "\n",
        "            corrected_tokens.append(best)\n",
        "        else:\n",
        "            corrected_tokens.append(token)\n",
        "\n",
        "    return ' '.join(corrected_tokens).replace(\" .\", \".\")\n",
        "\n",
        "# Test sentence\n",
        "test_sentence = \"Th quik braown foxs jmups ovem te lagzy qog.\"\n",
        "final_clean_result_fixed = hybrid_correction_clean(test_sentence, vocabulary)\n",
        "final_clean_result_fixed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "83IDpBKTeSDr",
        "outputId": "d2853308-1703-41f6-ac68-d3b1beb84896"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the quirk brawn foxes jumps hove tie lazy quashes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict, Counter\n",
        "from difflib import get_close_matches\n",
        "\n",
        "# Load vocabulary (keep as requested)\n",
        "vocab_path = \"/content/Vocabulary.txt\"\n",
        "with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    vocab_words = [line.strip().split()[0].lower() for line in f if line.strip() and line[0].isascii()]\n",
        "    vocabulary = set(vocab_words)\n",
        "\n",
        "# Build n-gram model (unigram + bigram + trigram)\n",
        "def build_ngram_model(vocab_list):\n",
        "    unigram = Counter(vocab_list)\n",
        "    bigram = defaultdict(lambda: defaultdict(int))\n",
        "    trigram = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
        "\n",
        "    for i in range(len(vocab_list) - 2):\n",
        "        w1, w2, w3 = vocab_list[i], vocab_list[i+1], vocab_list[i+2]\n",
        "        unigram[w1] += 1\n",
        "        bigram[w1][w2] += 1\n",
        "        trigram[w1][w2][w3] += 1\n",
        "\n",
        "    # Add final two for completeness\n",
        "    if len(vocab_list) >= 2:\n",
        "        bigram[vocab_list[-2]][vocab_list[-1]] += 1\n",
        "        unigram[vocab_list[-1]] += 1\n",
        "\n",
        "    return unigram, bigram, trigram\n",
        "\n",
        "unigram_model, bigram_model, trigram_model = build_ngram_model(vocab_words)\n",
        "\n",
        "# N-gram scoring function\n",
        "def ngram_score(words):\n",
        "    score = 0\n",
        "    for i in range(len(words)):\n",
        "        if i >= 2:\n",
        "            w1, w2, w3 = words[i-2], words[i-1], words[i]\n",
        "            score += trigram_model[w1][w2].get(w3, 0) * 3\n",
        "        elif i == 1:\n",
        "            w1, w2 = words[i-1], words[i]\n",
        "            score += bigram_model[w1].get(w2, 0) * 2\n",
        "        else:\n",
        "            w1 = words[i]\n",
        "            score += unigram_model.get(w1, 0)\n",
        "    return score\n",
        "\n",
        "# Generate correction candidates using difflib\n",
        "def generate_candidates(word, vocab, n=5, cutoff=0.75):\n",
        "    return get_close_matches(word, vocab, n=n, cutoff=cutoff)\n",
        "\n",
        "# Final context-aware correction function (with enhanced n-gram model)\n",
        "def context_aware_ngram_correction(sentence, vocab):\n",
        "    tokens = re.findall(r\"\\b\\w+\\b|[^\\w\\s]\", sentence)\n",
        "    corrected_tokens = []\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token.isalpha():\n",
        "            token_lower = token.lower()\n",
        "            candidates = generate_candidates(token_lower, vocab, n=5, cutoff=0.75)\n",
        "            if not candidates:\n",
        "                corrected_tokens.append(token_lower)\n",
        "                continue\n",
        "\n",
        "            # Try each candidate and pick the best based on context-aware n-gram score\n",
        "            best_candidate = candidates[0]\n",
        "            best_score = -1\n",
        "\n",
        "            for cand in candidates:\n",
        "                # Build temporary sentence\n",
        "                trial_tokens = corrected_tokens + [cand] + tokens[i+1:]\n",
        "                trial_words = [t for t in trial_tokens if t.isalpha()]\n",
        "                score = ngram_score(trial_words)\n",
        "                if score > best_score:\n",
        "                    best_candidate = cand\n",
        "                    best_score = score\n",
        "\n",
        "            corrected_tokens.append(best_candidate)\n",
        "        else:\n",
        "            corrected_tokens.append(token)\n",
        "\n",
        "    return ' '.join(corrected_tokens).replace(\" .\", \".\")\n",
        "\n",
        "# Run the upgraded n-gram correction\n",
        "test_sentence = \"Th quik braown foxs jmups ovem te lagzy qog.\"\n",
        "enhanced_ngram_result = context_aware_ngram_correction(test_sentence, vocabulary)\n",
        "enhanced_ngram_result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hSV03mZDg47z",
        "outputId": "18e1e15c-83d6-44be-eaf6-95d477e6ef89"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the quirk brown foxes mumps wove te lazy qog.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}