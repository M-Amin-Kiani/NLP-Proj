{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **mohammad amin kiani 4003613052**"
      ],
      "metadata": {
        "id": "rixKWUy2xQZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# load movie_reviews Corpus from NLTK:"
      ],
      "metadata": {
        "id": "YULO3240Nmbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhKkbTBEfRDG",
        "outputId": "c861866a-0542-459d-a3f7-1e24f7906da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# پاکسازی کامل کش‌های خراب\n",
        "nltk.download('popular')\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All docs"
      ],
      "metadata": {
        "id": "pyr3hEsiYqhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# لیست مستندات و برچسب‌ها\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# درهم‌ریزی برای train/test\n",
        "random.shuffle(documents)\n",
        "\n",
        "# تعداد کل و توزیع برچسب‌ها\n",
        "print(f\"تعداد کل مستندات: {len(documents)}\")\n",
        "from collections import Counter\n",
        "print(\"توزیع برچسب‌ها:\", Counter([label for (text, label) in documents]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJcX_vIKcky4",
        "outputId": "9fc6dd2a-4cab-411d-88e6-4d66bba67de2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تعداد کل مستندات: 2000\n",
            "توزیع برچسب‌ها: Counter({'pos': 1000, 'neg': 1000})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature extraction"
      ],
      "metadata": {
        "id": "GhCwc2njVPSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# استخراج واژگان مهم\n",
        "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
        "word_features = list(all_words)[:2000]  # 2000 کلمه برتر\n",
        "\n",
        "# تابع استخراج ویژگی از متن\n",
        "def document_features(document):\n",
        "    words = set(document)\n",
        "    features = {}\n",
        "    for w in word_features:\n",
        "        features[f'contains({w})'] = (w in words)\n",
        "    return features\n",
        "\n",
        "# ایجاد مجموعه ویژگی‌ها\n",
        "featuresets = [(document_features(d), c) for (d, c) in documents]\n"
      ],
      "metadata": {
        "id": "_znGpDWgVL3G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_words = set(stopwords.words('english'))\n",
        "# punct = set(string.punctuation)\n",
        "\n",
        "# #   پیش‌پردازش :\n",
        "# def clean_tokens(tokens):\n",
        "#     return [\n",
        "#         w.lower() for w in tokens\n",
        "#         if w.lower() not in stop_words              # حذف stopword\n",
        "#         and w not in punct                          # حذف علائم نگارشی\n",
        "#         and len(w) > 1                              # حذف کلمات خیلی کوتاه\n",
        "#         and w.isalpha()                             # فقط حروف الفبا\n",
        "#         and not re.match(r'https?://\\S+|\\d+', w)    # حذف URL و عدد\n",
        "#     ]\n",
        "\n",
        "# # بارگذاری داده‌ها:\n",
        "# documents = []\n",
        "# for category in movie_reviews.categories():\n",
        "#     for fileid in movie_reviews.fileids(category):\n",
        "#         words = clean_tokens(movie_reviews.words(fileid))\n",
        "#         documents.append((words, category))\n",
        "\n",
        "# random.shuffle(documents)\n",
        "\n",
        "#  توقف، علائم،  regex\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punct = set(string.punctuation)\n",
        "\n",
        "# تابع پیش‌پردازش\n",
        "def clean_tokens(tokens):\n",
        "    return [\n",
        "        w.lower() for w in tokens\n",
        "        if w.lower() not in stop_words\n",
        "        and w not in punct\n",
        "        and len(w) > 2\n",
        "        and w.isalpha()\n",
        "        and not re.match(r'https?://\\S+|\\d+', w)\n",
        "    ]\n",
        "\n",
        "# بارگذاری داده‌ها\n",
        "documents = []\n",
        "for category in movie_reviews.categories():\n",
        "    for fileid in movie_reviews.fileids(category):\n",
        "        words = clean_tokens(movie_reviews.words(fileid))\n",
        "        documents.append((words, category))\n",
        "\n",
        "random.shuffle(documents)\n"
      ],
      "metadata": {
        "id": "KTShx9IAEIqI"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lookout dataset"
      ],
      "metadata": {
        "id": "WIWaFlDiMBCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "punct = set(string.punctuation)\n",
        "custom_garbage = {'just', 'really', 'get', 'got', 'even', 'thing', 'things', 'also', 'though'}\n",
        "\n",
        "# پیش‌پردازش کامل\n",
        "def clean_tokens(tokens):\n",
        "    return [\n",
        "        w.lower() for w in tokens\n",
        "        if w.lower() not in stop_words\n",
        "        and w.lower() not in custom_garbage\n",
        "        and w not in punct\n",
        "        and len(w) > 2\n",
        "        and w.isalpha()\n",
        "        and not re.match(r'https?://\\S+|\\d+', w)\n",
        "    ]\n",
        "\n",
        "# بارگذاری داده‌ها\n",
        "documents = []\n",
        "for category in movie_reviews.categories():\n",
        "    for fileid in movie_reviews.fileids(category):\n",
        "        words = clean_tokens(movie_reviews.words(fileid))\n",
        "        documents.append((words, category))\n",
        "\n",
        "random.shuffle(documents)"
      ],
      "metadata": {
        "id": "qQC3MY8LMAR5"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model"
      ],
      "metadata": {
        "id": "EjdSD5c7VTys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "آماده:"
      ],
      "metadata": {
        "id": "OYSrg974EfmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.classify.util import accuracy\n",
        "\n",
        "# تقسیم آموزش/تست\n",
        "train_set, test_set = featuresets[:1600], featuresets[1600:]\n",
        "\n",
        "# آموزش مدل\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# دقت مدل\n",
        "print(\"Accuracy:\", accuracy(classifier, test_set))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88JPpNK0OLBa",
        "outputId": "dace5e14-1da6-4f2c-b0db-606f317b6a52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "دستی:"
      ],
      "metadata": {
        "id": "TEUwXbfpEdTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # تقسیم آموزش/تست\n",
        "# split_idx = int(0.7 * len(documents))\n",
        "# train_docs = documents[:split_idx]\n",
        "# test_docs = documents[split_idx:]\n",
        "\n",
        "# # مدل‌سازی\n",
        "# class_word_counts = {'pos': Counter(), 'neg': Counter()}\n",
        "# class_doc_counts = {'pos': 0, 'neg': 0}\n",
        "# total_vocab = Counter()\n",
        "\n",
        "# for words, label in train_docs:\n",
        "#     class_doc_counts[label] += 1\n",
        "#     class_word_counts[label].update(words)\n",
        "#     total_vocab.update(words)\n",
        "\n",
        "# # فیلتر کلمات نادر از واژگان\n",
        "# vocab = {w for w, c in total_vocab.items() if c >= 3}\n",
        "# V = len(vocab)\n",
        "\n",
        "# # به‌روزرسانی مدل‌ها بعد از حذف کلمات نادر\n",
        "# for cls in ['pos', 'neg']:\n",
        "#     class_word_counts[cls] = Counter({w: c for w, c in class_word_counts[cls].items() if w in vocab})\n",
        "\n",
        "# تقسیم به آموزش/تست\n",
        "split_idx = int(0.7 * len(documents))\n",
        "train_docs = documents[:split_idx]\n",
        "test_docs = documents[split_idx:]\n",
        "\n",
        "# ساخت واژگان و شمارش اولیه\n",
        "class_word_counts = {'pos': Counter(), 'neg': Counter()}\n",
        "class_doc_counts = {'pos': 0, 'neg': 0}\n",
        "total_vocab = Counter()\n",
        "\n",
        "for words, label in train_docs:\n",
        "    class_doc_counts[label] += 1\n",
        "    class_word_counts[label].update(words)\n",
        "    total_vocab.update(words)\n",
        "\n",
        "# فیلتر واژه‌های کم‌تکرار\n",
        "vocab = {w for w, c in total_vocab.items() if c >= 3}\n",
        "# محدود به 4000 واژه مهم\n",
        "top_words = set([w for w, c in total_vocab.most_common(4000)])\n",
        "vocab = vocab.intersection(top_words)\n",
        "V = len(vocab)\n",
        "\n",
        "# به‌روزرسانی مدل بر اساس واژگان بهبود یافته\n",
        "for cls in ['pos', 'neg']:\n",
        "    class_word_counts[cls] = Counter({w: c for w, c in class_word_counts[cls].items() if w in vocab})\n",
        "\n",
        "print(V)\n",
        "print(\"-----\")\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCR6emKwEZB4",
        "outputId": "75a63295-4f3d-44fc-843d-06a9f2434204"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4000\n",
            "-----\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'secretly',\n",
              " 'strange',\n",
              " 'officer',\n",
              " 'mia',\n",
              " 'great',\n",
              " 'imaginative',\n",
              " 'clever',\n",
              " 'develops',\n",
              " 'excitement',\n",
              " 'hundred',\n",
              " 'edition',\n",
              " 'catches',\n",
              " 'halloween',\n",
              " 'lake',\n",
              " 'accept',\n",
              " 'unlikely',\n",
              " 'else',\n",
              " 'emily',\n",
              " 'achieved',\n",
              " 'stranger',\n",
              " 'sentimental',\n",
              " 'perfectly',\n",
              " 'charismatic',\n",
              " 'levels',\n",
              " 'jawbreaker',\n",
              " 'wahlberg',\n",
              " 'creates',\n",
              " 'portrayal',\n",
              " 'patch',\n",
              " 'letter',\n",
              " 'ways',\n",
              " 'nightmare',\n",
              " 'thoughts',\n",
              " 'robbery',\n",
              " 'clooney',\n",
              " 'fantasies',\n",
              " 'soul',\n",
              " 'imagery',\n",
              " 'saving',\n",
              " 'depp',\n",
              " 'cameo',\n",
              " 'wonderful',\n",
              " 'fits',\n",
              " 'admit',\n",
              " 'lit',\n",
              " 'suffer',\n",
              " 'native',\n",
              " 'battle',\n",
              " 'casting',\n",
              " 'first',\n",
              " 'christopher',\n",
              " 'alice',\n",
              " 'sat',\n",
              " 'searching',\n",
              " 'jennifer',\n",
              " 'eddie',\n",
              " 'obsession',\n",
              " 'remarkably',\n",
              " 'cannot',\n",
              " 'turn',\n",
              " 'drop',\n",
              " 'tree',\n",
              " 'research',\n",
              " 'tape',\n",
              " 'planning',\n",
              " 'ads',\n",
              " 'battles',\n",
              " 'evening',\n",
              " 'lifetime',\n",
              " 'edge',\n",
              " 'todd',\n",
              " 'blake',\n",
              " 'bring',\n",
              " 'expecting',\n",
              " 'execution',\n",
              " 'sorts',\n",
              " 'tall',\n",
              " 'bacon',\n",
              " 'hannah',\n",
              " 'friendly',\n",
              " 'cop',\n",
              " 'shall',\n",
              " 'bottle',\n",
              " 'patients',\n",
              " 'reviews',\n",
              " 'god',\n",
              " 'predictable',\n",
              " 'heavy',\n",
              " 'reading',\n",
              " 'analyze',\n",
              " 'producing',\n",
              " 'animal',\n",
              " 'simon',\n",
              " 'overdone',\n",
              " 'mind',\n",
              " 'audiences',\n",
              " 'happen',\n",
              " 'included',\n",
              " 'month',\n",
              " 'hard',\n",
              " 'crashes',\n",
              " 'trapped',\n",
              " 'journalist',\n",
              " 'able',\n",
              " 'thinking',\n",
              " 'novel',\n",
              " 'rachel',\n",
              " 'florida',\n",
              " 'carry',\n",
              " 'includes',\n",
              " 'lumumba',\n",
              " 'harrelson',\n",
              " 'cares',\n",
              " 'extra',\n",
              " 'stereotypical',\n",
              " 'sister',\n",
              " 'screenwriters',\n",
              " 'dicaprio',\n",
              " 'romance',\n",
              " 'progresses',\n",
              " 'points',\n",
              " 'noticed',\n",
              " 'shine',\n",
              " 'count',\n",
              " 'sgt',\n",
              " 'last',\n",
              " 'courtney',\n",
              " 'perfection',\n",
              " 'reputation',\n",
              " 'irrelevant',\n",
              " 'usual',\n",
              " 'mrs',\n",
              " 'dull',\n",
              " 'post',\n",
              " 'hole',\n",
              " 'terrifying',\n",
              " 'starred',\n",
              " 'realm',\n",
              " 'perspective',\n",
              " 'government',\n",
              " 'newton',\n",
              " 'sheriff',\n",
              " 'school',\n",
              " 'sixth',\n",
              " 'lands',\n",
              " 'themes',\n",
              " 'honor',\n",
              " 'charlize',\n",
              " 'picture',\n",
              " 'word',\n",
              " 'closer',\n",
              " 'unlike',\n",
              " 'commentary',\n",
              " 'type',\n",
              " 'box',\n",
              " 'installment',\n",
              " 'charge',\n",
              " 'already',\n",
              " 'fair',\n",
              " 'know',\n",
              " 'heroin',\n",
              " 'photography',\n",
              " 'somewhat',\n",
              " 'remembered',\n",
              " 'teeth',\n",
              " 'selling',\n",
              " 'hearted',\n",
              " 'stretch',\n",
              " 'bed',\n",
              " 'disaster',\n",
              " 'owns',\n",
              " 'bought',\n",
              " 'months',\n",
              " 'interview',\n",
              " 'squad',\n",
              " 'angeles',\n",
              " 'jesus',\n",
              " 'breaking',\n",
              " 'status',\n",
              " 'makers',\n",
              " 'burning',\n",
              " 'surrounding',\n",
              " 'ear',\n",
              " 'gift',\n",
              " 'distance',\n",
              " 'previews',\n",
              " 'engaging',\n",
              " 'lets',\n",
              " 'extended',\n",
              " 'sake',\n",
              " 'similar',\n",
              " 'island',\n",
              " 'practically',\n",
              " 'illegal',\n",
              " 'ten',\n",
              " 'aside',\n",
              " 'lover',\n",
              " 'race',\n",
              " 'bull',\n",
              " 'bare',\n",
              " 'keeps',\n",
              " 'mildly',\n",
              " 'star',\n",
              " 'string',\n",
              " 'premise',\n",
              " 'investigator',\n",
              " 'exchange',\n",
              " 'feel',\n",
              " 'relate',\n",
              " 'scheme',\n",
              " 'clear',\n",
              " 'knight',\n",
              " 'notice',\n",
              " 'discovery',\n",
              " 'taran',\n",
              " 'kilmer',\n",
              " 'increasingly',\n",
              " 'times',\n",
              " 'filmed',\n",
              " 'profanity',\n",
              " 'lonely',\n",
              " 'high',\n",
              " 'redeeming',\n",
              " 'generation',\n",
              " 'cameos',\n",
              " 'folks',\n",
              " 'six',\n",
              " 'humor',\n",
              " 'help',\n",
              " 'rule',\n",
              " 'student',\n",
              " 'vince',\n",
              " 'murders',\n",
              " 'worn',\n",
              " 'sad',\n",
              " 'women',\n",
              " 'drew',\n",
              " 'study',\n",
              " 'soundtrack',\n",
              " 'breasts',\n",
              " 'bet',\n",
              " 'cinematographer',\n",
              " 'luck',\n",
              " 'meg',\n",
              " 'hint',\n",
              " 'naked',\n",
              " 'finally',\n",
              " 'information',\n",
              " 'america',\n",
              " 'geoffrey',\n",
              " 'lower',\n",
              " 'telling',\n",
              " 'raging',\n",
              " 'liz',\n",
              " 'suppose',\n",
              " 'singer',\n",
              " 'deeply',\n",
              " 'enjoyment',\n",
              " 'vacation',\n",
              " 'reach',\n",
              " 'studios',\n",
              " 'dangerous',\n",
              " 'dillon',\n",
              " 'convicted',\n",
              " 'official',\n",
              " 'way',\n",
              " 'crossing',\n",
              " 'proceeds',\n",
              " 'urban',\n",
              " 'told',\n",
              " 'adds',\n",
              " 'generally',\n",
              " 'mid',\n",
              " 'history',\n",
              " 'okay',\n",
              " 'human',\n",
              " 'opens',\n",
              " 'academy',\n",
              " 'brothers',\n",
              " 'quickly',\n",
              " 'impossible',\n",
              " 'appeal',\n",
              " 'elaborate',\n",
              " 'eastwood',\n",
              " 'acclaimed',\n",
              " 'separate',\n",
              " 'ignore',\n",
              " 'despite',\n",
              " 'stolen',\n",
              " 'cole',\n",
              " 'learns',\n",
              " 'pay',\n",
              " 'finest',\n",
              " 'nowhere',\n",
              " 'guilty',\n",
              " 'book',\n",
              " 'market',\n",
              " 'frankly',\n",
              " 'dressed',\n",
              " 'learned',\n",
              " 'matt',\n",
              " 'boyfriend',\n",
              " 'mild',\n",
              " 'mystical',\n",
              " 'threatening',\n",
              " 'johnny',\n",
              " 'bridge',\n",
              " 'stop',\n",
              " 'mother',\n",
              " 'hurt',\n",
              " 'thomas',\n",
              " 'wooden',\n",
              " 'serial',\n",
              " 'chuckle',\n",
              " 'supposed',\n",
              " 'sleepy',\n",
              " 'commercials',\n",
              " 'realize',\n",
              " 'devil',\n",
              " 'walking',\n",
              " 'playing',\n",
              " 'zero',\n",
              " 'among',\n",
              " 'mansion',\n",
              " 'people',\n",
              " 'flight',\n",
              " 'placed',\n",
              " 'imagination',\n",
              " 'mary',\n",
              " 'concept',\n",
              " 'reaction',\n",
              " 'utter',\n",
              " 'constant',\n",
              " 'seems',\n",
              " 'author',\n",
              " 'recently',\n",
              " 'ellen',\n",
              " 'subtle',\n",
              " 'bloody',\n",
              " 'size',\n",
              " 'millionaire',\n",
              " 'requires',\n",
              " 'brilliantly',\n",
              " 'proceedings',\n",
              " 'hat',\n",
              " 'august',\n",
              " 'wrong',\n",
              " 'zane',\n",
              " 'suck',\n",
              " 'middle',\n",
              " 'match',\n",
              " 'donnell',\n",
              " 'mature',\n",
              " 'fugitive',\n",
              " 'meet',\n",
              " 'upper',\n",
              " 'three',\n",
              " 'normal',\n",
              " 'advance',\n",
              " 'menacing',\n",
              " 'pierce',\n",
              " 'lying',\n",
              " 'smart',\n",
              " 'minds',\n",
              " 'titled',\n",
              " 'today',\n",
              " 'futuristic',\n",
              " 'loved',\n",
              " 'nation',\n",
              " 'argue',\n",
              " 'brilliant',\n",
              " 'tremendous',\n",
              " 'guy',\n",
              " 'court',\n",
              " 'kiki',\n",
              " 'sympathy',\n",
              " 'outside',\n",
              " 'con',\n",
              " 'charisma',\n",
              " 'anne',\n",
              " 'combat',\n",
              " 'action',\n",
              " 'writer',\n",
              " 'power',\n",
              " 'wondering',\n",
              " 'doom',\n",
              " 'using',\n",
              " 'joke',\n",
              " 'massive',\n",
              " 'provoking',\n",
              " 'response',\n",
              " 'true',\n",
              " 'vision',\n",
              " 'kid',\n",
              " 'clerk',\n",
              " 'nightclub',\n",
              " 'odd',\n",
              " 'bother',\n",
              " 'total',\n",
              " 'follows',\n",
              " 'books',\n",
              " 'echoes',\n",
              " 'cuba',\n",
              " 'example',\n",
              " 'cage',\n",
              " 'ripley',\n",
              " 'iii',\n",
              " 'appearances',\n",
              " 'fail',\n",
              " 'young',\n",
              " 'minutes',\n",
              " 'obnoxious',\n",
              " 'villain',\n",
              " 'join',\n",
              " 'andrew',\n",
              " 'fish',\n",
              " 'goodman',\n",
              " 'technical',\n",
              " 'peak',\n",
              " 'created',\n",
              " 'agrees',\n",
              " 'circumstances',\n",
              " 'shrek',\n",
              " 'lends',\n",
              " 'uplifting',\n",
              " 'loves',\n",
              " 'stereotypes',\n",
              " 'costs',\n",
              " 'bowling',\n",
              " 'rest',\n",
              " 'hearing',\n",
              " 'village',\n",
              " 'spirit',\n",
              " 'rain',\n",
              " 'stigmata',\n",
              " 'ultra',\n",
              " 'hitting',\n",
              " 'mtv',\n",
              " 'mix',\n",
              " 'ladies',\n",
              " 'rated',\n",
              " 'screaming',\n",
              " 'conspiracy',\n",
              " 'somebody',\n",
              " 'unpleasant',\n",
              " 'dramatic',\n",
              " 'original',\n",
              " 'loyal',\n",
              " 'outstanding',\n",
              " 'seven',\n",
              " 'packed',\n",
              " 'spiritual',\n",
              " 'moment',\n",
              " 'conceived',\n",
              " 'ready',\n",
              " 'scene',\n",
              " 'crucial',\n",
              " 'dreams',\n",
              " 'edward',\n",
              " 'notable',\n",
              " 'pat',\n",
              " 'however',\n",
              " 'broderick',\n",
              " 'club',\n",
              " 'lesson',\n",
              " 'straightforward',\n",
              " 'say',\n",
              " 'delivery',\n",
              " 'jumping',\n",
              " 'kudrow',\n",
              " 'countless',\n",
              " 'hate',\n",
              " 'going',\n",
              " 'adapted',\n",
              " 'judging',\n",
              " 'ran',\n",
              " 'wit',\n",
              " 'austin',\n",
              " 'agent',\n",
              " 'studio',\n",
              " 'blond',\n",
              " 'knows',\n",
              " 'guts',\n",
              " 'actor',\n",
              " 'prepared',\n",
              " 'either',\n",
              " 'cut',\n",
              " 'figure',\n",
              " 'board',\n",
              " 'beat',\n",
              " 'titanic',\n",
              " 'raw',\n",
              " 'scorsese',\n",
              " 'medical',\n",
              " 'bugs',\n",
              " 'debut',\n",
              " 'although',\n",
              " 'importantly',\n",
              " 'skip',\n",
              " 'career',\n",
              " 'mistake',\n",
              " 'awards',\n",
              " 'hereafter',\n",
              " 'comedian',\n",
              " 'perfect',\n",
              " 'danger',\n",
              " 'linda',\n",
              " 'freeze',\n",
              " 'season',\n",
              " 'developments',\n",
              " 'effects',\n",
              " 'added',\n",
              " 'stuck',\n",
              " 'caring',\n",
              " 'soldiers',\n",
              " 'mainstream',\n",
              " 'enjoyable',\n",
              " 'branagh',\n",
              " 'principal',\n",
              " 'walsh',\n",
              " 'basis',\n",
              " 'glenn',\n",
              " 'hero',\n",
              " 'jordan',\n",
              " 'prevent',\n",
              " 'dancer',\n",
              " 'lucas',\n",
              " 'yeah',\n",
              " 'accidentally',\n",
              " 'beverly',\n",
              " 'vice',\n",
              " 'simple',\n",
              " 'desperately',\n",
              " 'wasted',\n",
              " 'crazy',\n",
              " 'succeeds',\n",
              " 'random',\n",
              " 'back',\n",
              " 'growing',\n",
              " 'cameras',\n",
              " 'boss',\n",
              " 'combined',\n",
              " 'disbelief',\n",
              " 'clues',\n",
              " 'stupid',\n",
              " 'mystery',\n",
              " 'personality',\n",
              " 'waiting',\n",
              " 'passion',\n",
              " 'explaining',\n",
              " 'busy',\n",
              " 'changed',\n",
              " 'sports',\n",
              " 'tedious',\n",
              " 'miles',\n",
              " 'accepts',\n",
              " 'schwarzenegger',\n",
              " 'especially',\n",
              " 'emma',\n",
              " 'buck',\n",
              " 'provide',\n",
              " 'shower',\n",
              " 'woods',\n",
              " 'main',\n",
              " 'gabriel',\n",
              " 'arrival',\n",
              " 'kill',\n",
              " 'pure',\n",
              " 'directs',\n",
              " 'key',\n",
              " 'composed',\n",
              " 'instinct',\n",
              " 'moved',\n",
              " 'egoyan',\n",
              " 'need',\n",
              " 'attempting',\n",
              " 'revelation',\n",
              " 'devices',\n",
              " 'command',\n",
              " 'slightest',\n",
              " 'sweetback',\n",
              " 'intent',\n",
              " 'loud',\n",
              " 'monica',\n",
              " 'realistic',\n",
              " 'difference',\n",
              " 'nature',\n",
              " 'nbsp',\n",
              " 'develop',\n",
              " 'annie',\n",
              " 'saves',\n",
              " 'creation',\n",
              " 'contained',\n",
              " 'mob',\n",
              " 'metal',\n",
              " 'malick',\n",
              " 'goes',\n",
              " 'mixed',\n",
              " 'aboard',\n",
              " 'delivering',\n",
              " 'footage',\n",
              " 'wes',\n",
              " 'recommend',\n",
              " 'try',\n",
              " 'acting',\n",
              " 'office',\n",
              " 'releases',\n",
              " 'dozens',\n",
              " 'towards',\n",
              " 'dry',\n",
              " 'justice',\n",
              " 'virus',\n",
              " 'sisters',\n",
              " 'reindeer',\n",
              " 'families',\n",
              " 'braveheart',\n",
              " 'lee',\n",
              " 'frustrated',\n",
              " 'nonetheless',\n",
              " 'revealing',\n",
              " 'hidden',\n",
              " 'creative',\n",
              " 'warner',\n",
              " 'images',\n",
              " 'kind',\n",
              " 'helen',\n",
              " 'driver',\n",
              " 'awesome',\n",
              " 'complex',\n",
              " 'chucky',\n",
              " 'remember',\n",
              " 'third',\n",
              " 'exploring',\n",
              " 'tongue',\n",
              " 'speaking',\n",
              " 'relief',\n",
              " 'goal',\n",
              " 'neo',\n",
              " 'albert',\n",
              " 'ugly',\n",
              " 'tears',\n",
              " 'guess',\n",
              " 'certain',\n",
              " 'characterization',\n",
              " 'equal',\n",
              " 'editing',\n",
              " 'forgotten',\n",
              " 'impress',\n",
              " 'robbins',\n",
              " 'rock',\n",
              " 'theme',\n",
              " 'terrorist',\n",
              " 'attorney',\n",
              " 'psycho',\n",
              " 'bunch',\n",
              " 'ensues',\n",
              " 'destroying',\n",
              " 'cast',\n",
              " 'drive',\n",
              " 'sub',\n",
              " 'endings',\n",
              " 'chan',\n",
              " 'focuses',\n",
              " 'ricci',\n",
              " 'adam',\n",
              " 'possessed',\n",
              " 'professor',\n",
              " 'art',\n",
              " 'fire',\n",
              " 'harsh',\n",
              " 'knew',\n",
              " 'case',\n",
              " 'jerry',\n",
              " 'green',\n",
              " 'named',\n",
              " 'madness',\n",
              " 'viewer',\n",
              " 'promised',\n",
              " 'judd',\n",
              " 'tells',\n",
              " 'taylor',\n",
              " 'decisions',\n",
              " 'profession',\n",
              " 'walker',\n",
              " 'flashbacks',\n",
              " 'laughed',\n",
              " 'english',\n",
              " 'soon',\n",
              " 'effect',\n",
              " 'pacing',\n",
              " 'heroine',\n",
              " 'attention',\n",
              " 'fake',\n",
              " 'force',\n",
              " 'murder',\n",
              " 'dream',\n",
              " 'chosen',\n",
              " 'bride',\n",
              " 'firm',\n",
              " 'chose',\n",
              " 'joblo',\n",
              " 'problem',\n",
              " 'background',\n",
              " 'allows',\n",
              " 'thoroughly',\n",
              " 'patient',\n",
              " 'seconds',\n",
              " 'numbers',\n",
              " 'craven',\n",
              " 'worry',\n",
              " 'dean',\n",
              " 'content',\n",
              " 'viewed',\n",
              " 'tie',\n",
              " 'legends',\n",
              " 'hanks',\n",
              " 'discovered',\n",
              " 'tell',\n",
              " 'screening',\n",
              " 'destiny',\n",
              " 'five',\n",
              " 'former',\n",
              " 'revenge',\n",
              " 'jim',\n",
              " 'unique',\n",
              " 'wears',\n",
              " 'tear',\n",
              " 'ultimate',\n",
              " 'discover',\n",
              " 'greater',\n",
              " 'surviving',\n",
              " 'western',\n",
              " 'hardly',\n",
              " 'generated',\n",
              " 'gag',\n",
              " 'inspector',\n",
              " 'summer',\n",
              " 'influence',\n",
              " 'beings',\n",
              " 'peace',\n",
              " 'wants',\n",
              " 'experiment',\n",
              " 'ghosts',\n",
              " 'watchable',\n",
              " 'jumps',\n",
              " 'stock',\n",
              " 'quest',\n",
              " 'afraid',\n",
              " 'josh',\n",
              " 'pursuit',\n",
              " 'unrealistic',\n",
              " 'including',\n",
              " 'respective',\n",
              " 'attacks',\n",
              " 'liked',\n",
              " 'killers',\n",
              " 'nearly',\n",
              " 'fairy',\n",
              " 'think',\n",
              " 'adams',\n",
              " 'plays',\n",
              " 'shows',\n",
              " 'appeared',\n",
              " 'visually',\n",
              " 'thrill',\n",
              " 'altogether',\n",
              " 'smooth',\n",
              " 'excellent',\n",
              " 'proper',\n",
              " 'independent',\n",
              " 'deuce',\n",
              " 'stays',\n",
              " 'across',\n",
              " 'beast',\n",
              " 'dollar',\n",
              " 'sidekick',\n",
              " 'highlight',\n",
              " 'returning',\n",
              " 'historical',\n",
              " 'virtually',\n",
              " 'johnson',\n",
              " 'code',\n",
              " 'ends',\n",
              " 'bright',\n",
              " 'spirited',\n",
              " 'decided',\n",
              " 'runs',\n",
              " 'bill',\n",
              " 'explanation',\n",
              " 'capsule',\n",
              " 'runner',\n",
              " 'fargo',\n",
              " 'strike',\n",
              " 'poignant',\n",
              " 'eerie',\n",
              " 'underground',\n",
              " 'wings',\n",
              " 'along',\n",
              " 'pointless',\n",
              " 'bruce',\n",
              " 'worked',\n",
              " 'intelligent',\n",
              " 'vader',\n",
              " 'comment',\n",
              " 'particularly',\n",
              " 'campbell',\n",
              " 'exact',\n",
              " 'design',\n",
              " 'verhoeven',\n",
              " 'crowe',\n",
              " 'supporting',\n",
              " 'raise',\n",
              " 'touches',\n",
              " 'signs',\n",
              " 'williamson',\n",
              " 'remote',\n",
              " 'still',\n",
              " 'roles',\n",
              " 'apostle',\n",
              " 'secretary',\n",
              " 'conclusion',\n",
              " 'destination',\n",
              " 'gave',\n",
              " 'fight',\n",
              " 'songs',\n",
              " 'elizabeth',\n",
              " 'incredible',\n",
              " 'killing',\n",
              " 'brain',\n",
              " 'encounter',\n",
              " 'warning',\n",
              " 'fiction',\n",
              " 'adults',\n",
              " 'hackman',\n",
              " 'marc',\n",
              " 'talk',\n",
              " 'greg',\n",
              " 'victory',\n",
              " 'dreyfuss',\n",
              " 'bank',\n",
              " 'plus',\n",
              " 'providing',\n",
              " 'view',\n",
              " 'filmmaker',\n",
              " 'rooms',\n",
              " 'sleazy',\n",
              " 'forty',\n",
              " 'dealt',\n",
              " 'spin',\n",
              " 'theatrical',\n",
              " 'bar',\n",
              " 'ashley',\n",
              " 'cops',\n",
              " 'lame',\n",
              " 'singing',\n",
              " 'affair',\n",
              " 'ann',\n",
              " 'encounters',\n",
              " 'die',\n",
              " 'nick',\n",
              " 'toys',\n",
              " 'superior',\n",
              " 'james',\n",
              " 'eye',\n",
              " 'possibly',\n",
              " 'max',\n",
              " 'location',\n",
              " 'blood',\n",
              " 'barry',\n",
              " 'memories',\n",
              " 'lacking',\n",
              " 'rich',\n",
              " 'effective',\n",
              " 'cope',\n",
              " 'previously',\n",
              " 'knife',\n",
              " 'michelle',\n",
              " 'offered',\n",
              " 'opportunities',\n",
              " 'ralph',\n",
              " 'prison',\n",
              " 'artistic',\n",
              " 'extraordinary',\n",
              " 'amusing',\n",
              " 'scares',\n",
              " 'believing',\n",
              " 'ambitious',\n",
              " 'steals',\n",
              " 'macho',\n",
              " 'butler',\n",
              " 'mortal',\n",
              " 'yes',\n",
              " 'fear',\n",
              " 'missing',\n",
              " 'heart',\n",
              " 'hammer',\n",
              " 'charming',\n",
              " 'convoluted',\n",
              " 'face',\n",
              " 'woo',\n",
              " 'positive',\n",
              " 'situations',\n",
              " 'sandler',\n",
              " 'enjoy',\n",
              " 'target',\n",
              " 'vincent',\n",
              " 'costume',\n",
              " 'caused',\n",
              " 'bigger',\n",
              " 'professional',\n",
              " 'driving',\n",
              " 'wannabe',\n",
              " 'vegas',\n",
              " 'gritty',\n",
              " 'spend',\n",
              " 'chow',\n",
              " 'kick',\n",
              " 'guessed',\n",
              " 'act',\n",
              " 'woody',\n",
              " 'creepy',\n",
              " 'ape',\n",
              " 'accused',\n",
              " 'mexico',\n",
              " 'week',\n",
              " 'closely',\n",
              " 'religious',\n",
              " 'comparisons',\n",
              " 'recognize',\n",
              " 'learn',\n",
              " 'willis',\n",
              " 'stunt',\n",
              " 'robots',\n",
              " 'ability',\n",
              " 'contains',\n",
              " 'sole',\n",
              " 'roman',\n",
              " 'aged',\n",
              " 'alas',\n",
              " 'tea',\n",
              " 'stephen',\n",
              " 'flicks',\n",
              " 'restaurant',\n",
              " 'suggests',\n",
              " 'producer',\n",
              " 'slowly',\n",
              " 'exactly',\n",
              " 'baseball',\n",
              " 'ground',\n",
              " 'sexy',\n",
              " 'slapstick',\n",
              " 'hype',\n",
              " 'gladiator',\n",
              " 'cook',\n",
              " 'marshall',\n",
              " 'burst',\n",
              " 'mate',\n",
              " 'speech',\n",
              " 'anthony',\n",
              " 'lucky',\n",
              " 'committed',\n",
              " 'achieve',\n",
              " 'originality',\n",
              " 'hospital',\n",
              " 'individuals',\n",
              " 'building',\n",
              " 'gorgeous',\n",
              " 'would',\n",
              " 'collection',\n",
              " 'arms',\n",
              " 'appealing',\n",
              " 'coming',\n",
              " 'jedi',\n",
              " 'alex',\n",
              " 'matter',\n",
              " 'richard',\n",
              " 'realizing',\n",
              " 'hell',\n",
              " 'finish',\n",
              " 'wire',\n",
              " 'police',\n",
              " 'fred',\n",
              " 'spectacle',\n",
              " 'obvious',\n",
              " 'sincere',\n",
              " 'year',\n",
              " 'deliver',\n",
              " 'crash',\n",
              " 'disco',\n",
              " 'banderas',\n",
              " 'joy',\n",
              " 'visits',\n",
              " 'card',\n",
              " 'alcoholic',\n",
              " 'twister',\n",
              " 'nice',\n",
              " 'long',\n",
              " 'record',\n",
              " 'fans',\n",
              " 'angry',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def predict_naive_bayes(text_tokens):\n",
        "#     # پیش‌پردازش\n",
        "#     tokens = clean_tokens(text_tokens)\n",
        "\n",
        "#     scores = {}\n",
        "#     total_docs = sum(class_doc_counts.values())\n",
        "\n",
        "#     for cls in ['pos', 'neg']:\n",
        "#         # log(P(class))\n",
        "#         log_prob = math.log(class_doc_counts[cls] / total_docs)\n",
        "\n",
        "#         # log(P(w | class)) مجموعی\n",
        "#         for word in tokens:\n",
        "#             word_count = class_word_counts[cls][word]\n",
        "#             total_words = sum(class_word_counts[cls].values())\n",
        "#             prob = (word_count + 1) / (total_words + V)  # Laplace smoothing\n",
        "#             log_prob += math.log(prob)\n",
        "\n",
        "#         scores[cls] = log_prob\n",
        "\n",
        "#     return max(scores, key=scores.get)\n",
        "\n",
        "def predict_naive_bayes(text_tokens):\n",
        "    tokens = clean_tokens(text_tokens)\n",
        "    scores = {}\n",
        "    total_docs = sum(class_doc_counts.values())\n",
        "\n",
        "    for cls in ['pos', 'neg']:\n",
        "        log_prob = math.log(class_doc_counts[cls] / total_docs)\n",
        "        if cls == 'pos':\n",
        "            log_prob += 0.2  # تقویت prior کلاس مثبت\n",
        "\n",
        "        total_words = sum(class_word_counts[cls].values())\n",
        "\n",
        "        for word in tokens:\n",
        "            if word in vocab:\n",
        "                count = class_word_counts[cls][word]\n",
        "                numerator = math.log(count + 1)\n",
        "                denominator = math.log(total_words + V)\n",
        "                log_prob += numerator - denominator\n",
        "\n",
        "        scores[cls] = log_prob\n",
        "\n",
        "    return max(scores, key=scores.get)\n"
      ],
      "metadata": {
        "id": "kcItOsLdFs4Y"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import math\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for text, label in test_docs:\n",
        "    y_true.append(label)\n",
        "    y_pred.append(predict_naive_bayes(text))\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Precision:\", precision_score(y_true, y_pred, pos_label='pos'))\n",
        "print(\"Recall:\", recall_score(y_true, y_pred, pos_label='pos'))\n",
        "print(\"F1 Score:\", f1_score(y_true, y_pred, pos_label='pos'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hruC51KF2lQ",
        "outputId": "77f34ad4-a2d9-41c9-bca5-10c277ab8a7d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8166666666666667\n",
            "Precision: 0.8485915492957746\n",
            "Recall: 0.7824675324675324\n",
            "F1 Score: 0.8141891891891891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true, y_pred = [], []\n",
        "\n",
        "for words, label in test_docs:\n",
        "    y_true.append(label)\n",
        "    y_pred.append(predict_naive_bayes(words))\n",
        "\n",
        "print(classification_report(y_true, y_pred, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGwXBCEVIy15",
        "outputId": "3a873440-777c-401d-81b5-a9bfbc7d0345"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg     0.8249    0.8221    0.8235       298\n",
            "         pos     0.8251    0.8278    0.8264       302\n",
            "\n",
            "    accuracy                         0.8250       600\n",
            "   macro avg     0.8250    0.8250    0.8250       600\n",
            "weighted avg     0.8250    0.8250    0.8250       600\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval"
      ],
      "metadata": {
        "id": "fq5y1PwFTgyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# پیش‌بینی و مقایسه برچسب‌ها\n",
        "test_gold = [label for (features, label) in test_set]\n",
        "test_pred = [classifier.classify(features) for (features, label) in test_set]\n",
        "\n",
        "print(classification_report(test_gold, test_pred, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQhxQC95azgn",
        "outputId": "5f2ecb58-70a1-404b-bd66-58fda9ca0649"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg     0.8125    0.8895    0.8492       190\n",
            "         pos     0.8906    0.8143    0.8507       210\n",
            "\n",
            "    accuracy                         0.8500       400\n",
            "   macro avg     0.8516    0.8519    0.8500       400\n",
            "weighted avg     0.8535    0.8500    0.8500       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.show_most_informative_features(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeiaJUPt_NfP",
        "outputId": "f41981c5-bc30-4e86-b062-3c44092b5122"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "        contains(seagal) = True              neg : pos    =     11.4 : 1.0\n",
            "   contains(outstanding) = True              pos : neg    =      9.6 : 1.0\n",
            "         contains(damon) = True              pos : neg    =      7.6 : 1.0\n",
            "   contains(wonderfully) = True              pos : neg    =      5.9 : 1.0\n",
            "    contains(ridiculous) = True              neg : pos    =      5.4 : 1.0\n",
            "          contains(lame) = True              neg : pos    =      5.2 : 1.0\n",
            "        contains(wasted) = True              neg : pos    =      5.1 : 1.0\n",
            "         contains(awful) = True              neg : pos    =      5.0 : 1.0\n",
            "         contains(waste) = True              neg : pos    =      4.6 : 1.0\n",
            "         contains(flynt) = True              pos : neg    =      4.4 : 1.0\n"
          ]
        }
      ]
    }
  ]
}