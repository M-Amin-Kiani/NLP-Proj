{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z8CHvT6C-u5v",
        "outputId": "d1a841ee-ea67-4478-8cf6-b044709e3439"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm)\n",
            "  Downloading fasttext_wheel-0.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting flashtext<3.0,>=2.7 (from hazm)\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gensim<5.0.0,>=4.3.1 (from hazm)\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from hazm) (3.9.1)\n",
            "Collecting numpy==1.24.3 (from hazm)\n",
            "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting python-crfsuite<0.10.0,>=0.9.9 (from hazm)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from hazm) (1.6.1)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel<0.10.0,>=0.9.2->hazm)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (75.1.0)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim<5.0.0,>=4.3.1->hazm)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.67.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.17.2)\n",
            "Downloading hazm-0.10.0-py3-none-any.whl (892 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m892.6/892.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasttext_wheel-0.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9298 sha256=732cd240e48da6987c7ce74c6fea6c2b2b8105908e61d59c73133c96c8289a31\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/20/47/f03dfa8a7239c54cbc44ff7389eefbf888d2c1873edaaec888\n",
            "Successfully built flashtext\n",
            "Installing collected packages: flashtext, python-crfsuite, pybind11, numpy, scipy, fasttext-wheel, gensim, hazm\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "blosc2 3.2.0 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fasttext-wheel-0.9.2 flashtext-2.7 gensim-4.3.3 hazm-0.10.0 numpy-1.24.3 pybind11-2.13.6 python-crfsuite-0.9.11 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "d4a23d8b79734bc99d2b263f4e0b696d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import Normalizer\n",
        "import re\n",
        "\n",
        "words_to_numbers = {\n",
        "    \"ØµÙØ±\": 0,\n",
        "    \"ÛŒÚ©\": 1, \"Ø¯Ùˆ\": 2, \"Ø³Ù‡\": 3, \"Ú†Ù‡Ø§Ø±\": 4, \"Ù¾Ù†Ø¬\": 5,\n",
        "    \"Ø´Ø´\": 6, \"Ù‡ÙØª\": 7, \"Ù‡Ø´Øª\": 8, \"Ù†Ù‡\": 9,\n",
        "    \"Ø¯Ù‡\": 10, \"ÛŒØ§Ø²Ø¯Ù‡\": 11, \"Ø¯ÙˆØ§Ø²Ø¯Ù‡\": 12,\n",
        "    \"Ø³ÛŒØ²Ø¯Ù‡\": 13, \"Ú†Ù‡Ø§Ø±Ø¯Ù‡\": 14, \"Ù¾Ø§Ù†Ø²Ø¯Ù‡\": 15,\n",
        "    \"Ø´Ø§Ù†Ø²Ø¯Ù‡\": 16, \"Ù‡ÙØ¯Ù‡\": 17, \"Ù‡Ø¬Ø¯Ù‡\": 18, \"Ù†ÙˆØ²Ø¯Ù‡\": 19,\n",
        "    \"Ø¨ÛŒØ³Øª\": 20, \"Ø³ÛŒ\": 30, \"Ú†Ù‡Ù„\": 40, \"Ù¾Ù†Ø¬Ø§Ù‡\": 50,\n",
        "    \"Ø´ØµØª\": 60, \"Ù‡ÙØªØ§Ø¯\": 70, \"Ù‡Ø´ØªØ§Ø¯\": 80, \"Ù†ÙˆØ¯\": 90,\n",
        "    \"ØµØ¯\": 100, \"Ø¯ÙˆÛŒØ³Øª\": 200, \"Ø³ÛŒØµØ¯\": 300, \"Ú†Ù‡Ø§Ø±ØµØ¯\": 400,\n",
        "    \"Ù¾Ø§Ù†ØµØ¯\": 500, \"Ø´Ø´ØµØ¯\": 600, \"Ù‡ÙØªØµØ¯\": 700,\n",
        "    \"Ù‡Ø´ØªØµØ¯\": 800, \"Ù†Ù‡ØµØ¯\": 900\n",
        "}\n",
        "\n",
        "magnitudes = {\n",
        "    \"Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯\": 1000000000,\n",
        "    \"Ù…ÛŒÙ„ÛŒÙˆÙ†\": 1000000,\n",
        "    \"Ù‡Ø²Ø§Ø±\": 1000\n",
        "}\n",
        "\n",
        "def convert_to_persian_number(number):\n",
        "    english_to_persian_digits = str.maketrans(\"0123456789\", \"Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹\")\n",
        "    return str(number).translate(english_to_persian_digits)\n",
        "\n",
        "def parse_simple_number(words):\n",
        "    total = 0\n",
        "    for word in words:\n",
        "        if word in words_to_numbers:\n",
        "            total += words_to_numbers[word]\n",
        "    return total\n",
        "\n",
        "def text_to_number_fa(text):\n",
        "    normalizer = Normalizer()\n",
        "    text = normalizer.normalize(text)\n",
        "    text = text.replace(\" Ùˆ \", \" \")\n",
        "    words = text.strip().split()\n",
        "\n",
        "    total = 0\n",
        "    current_chunk = []\n",
        "    current_magnitude = None\n",
        "\n",
        "    for word in words:\n",
        "        if word in magnitudes:\n",
        "            num = parse_simple_number(current_chunk)\n",
        "            total += num * magnitudes[word]\n",
        "            current_chunk = []\n",
        "        else:\n",
        "            current_chunk.append(word)\n",
        "\n",
        "    if current_chunk:\n",
        "        total += parse_simple_number(current_chunk)\n",
        "\n",
        "    return total\n",
        "\n",
        "# --- Ø§Ø¬Ø±Ø§ ---\n",
        "user_input = input(\"Ø¹Ø¯Ø¯ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: \")\n",
        "number_result = text_to_number_fa(user_input)\n",
        "persian_number = convert_to_persian_number(number_result)\n",
        "\n",
        "print(\" ÙˆØ±ÙˆØ¯ÛŒ :\", user_input)\n",
        "print(\" Ø®Ø±ÙˆØ¬ÛŒ Ø¹Ø¯Ø¯ÛŒ (ÙØ§Ø±Ø³ÛŒ):\", persian_number)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HaR_zdP_P_i",
        "outputId": "acd458e4-19e2-4220-8d70-5a8dcc3ae0bf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ø¹Ø¯Ø¯ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: Ø³Ù‡ Ù…ÛŒÙ„ÛŒÙˆÙ† Ùˆ Ø³ÛŒØµØ¯ Ùˆ Ù¾Ù†Ø¬Ø§Ù‡ Ùˆ Ø³Ù‡\t\n",
            " ÙˆØ±ÙˆØ¯ÛŒ : Ø³Ù‡ Ù…ÛŒÙ„ÛŒÙˆÙ† Ùˆ Ø³ÛŒØµØ¯ Ùˆ Ù¾Ù†Ø¬Ø§Ù‡ Ùˆ Ø³Ù‡\t\n",
            " Ø®Ø±ÙˆØ¬ÛŒ Ø¹Ø¯Ø¯ÛŒ (ÙØ§Ø±Ø³ÛŒ): Û³Û°Û°Û°Û³ÛµÛ³\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import Normalizer\n",
        "import re\n",
        "\n",
        "words_to_numbers = {\n",
        "    \"ØµÙØ±\": 0, \"ÛŒÚ©\": 1, \"Ø¯Ùˆ\": 2, \"Ø³Ù‡\": 3, \"Ú†Ù‡Ø§Ø±\": 4, \"Ù¾Ù†Ø¬\": 5,\n",
        "    \"Ø´Ø´\": 6, \"Ù‡ÙØª\": 7, \"Ù‡Ø´Øª\": 8, \"Ù†Ù‡\": 9,\n",
        "    \"Ø¯Ù‡\": 10, \"ÛŒØ§Ø²Ø¯Ù‡\": 11, \"Ø¯ÙˆØ§Ø²Ø¯Ù‡\": 12,\n",
        "    \"Ø³ÛŒØ²Ø¯Ù‡\": 13, \"Ú†Ù‡Ø§Ø±Ø¯Ù‡\": 14, \"Ù¾Ø§Ù†Ø²Ø¯Ù‡\": 15,\n",
        "    \"Ø´Ø§Ù†Ø²Ø¯Ù‡\": 16, \"Ù‡ÙØ¯Ù‡\": 17, \"Ù‡Ø¬Ø¯Ù‡\": 18, \"Ù†ÙˆØ²Ø¯Ù‡\": 19,\n",
        "    \"Ø¨ÛŒØ³Øª\": 20, \"Ø³ÛŒ\": 30, \"Ú†Ù‡Ù„\": 40, \"Ù¾Ù†Ø¬Ø§Ù‡\": 50,\n",
        "    \"Ø´ØµØª\": 60, \"Ù‡ÙØªØ§Ø¯\": 70, \"Ù‡Ø´ØªØ§Ø¯\": 80, \"Ù†ÙˆØ¯\": 90,\n",
        "    \"ØµØ¯\": 100, \"Ø¯ÙˆÛŒØ³Øª\": 200, \"Ø³ÛŒØµØ¯\": 300, \"Ú†Ù‡Ø§Ø±ØµØ¯\": 400,\n",
        "    \"Ù¾Ø§Ù†ØµØ¯\": 500, \"Ø´Ø´ØµØ¯\": 600, \"Ù‡ÙØªØµØ¯\": 700,\n",
        "    \"Ù‡Ø´ØªØµØ¯\": 800, \"Ù†Ù‡ØµØ¯\": 900\n",
        "}\n",
        "\n",
        "magnitudes = {\n",
        "    \"Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯\": 1000000000,\n",
        "    \"Ù…ÛŒÙ„ÛŒÙˆÙ†\": 1000000,\n",
        "    \"Ù‡Ø²Ø§Ø±\": 1000\n",
        "}\n",
        "\n",
        "all_keywords = list(words_to_numbers.keys()) + list(magnitudes.keys())\n",
        "\n",
        "def normalize_input(text):\n",
        "    normalizer = Normalizer()\n",
        "    text = normalizer.normalize(text)\n",
        "    text = text.replace(\"â€Œ\", \"\")  # Ø­Ø°Ù Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡\n",
        "    text = text.replace(\" \", \"\")  # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ\n",
        "\n",
        "    # ØªØ·Ø¨ÛŒÙ‚ ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø§Ø² Ø±ÙˆÛŒ ÙˆØ§Ú˜Ù‡â€ŒÙ†Ø§Ù…Ù‡\n",
        "    pattern = \"(\" + \"|\".join(sorted(all_keywords, key=len, reverse=True)) + \")\"\n",
        "    tokens = re.findall(pattern, text)\n",
        "    return tokens\n",
        "\n",
        "def convert_to_persian_number(number):\n",
        "    return str(number).translate(str.maketrans(\"0123456789\", \"Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹\"))\n",
        "\n",
        "def parse_simple_number(words):\n",
        "    return sum(words_to_numbers.get(word, 0) for word in words)\n",
        "\n",
        "def text_to_number_fa(text):\n",
        "    words = normalize_input(text)\n",
        "    total = 0\n",
        "    current_chunk = []\n",
        "\n",
        "    for word in words:\n",
        "        if word in magnitudes:\n",
        "            num = parse_simple_number(current_chunk)\n",
        "            total += num * magnitudes[word]\n",
        "            current_chunk = []\n",
        "        else:\n",
        "            current_chunk.append(word)\n",
        "\n",
        "    if current_chunk:\n",
        "        total += parse_simple_number(current_chunk)\n",
        "\n",
        "    return total\n",
        "\n",
        "# --- Ø§Ø¬Ø±Ø§ ---\n",
        "user_input = input(\"Ø¹Ø¯Ø¯ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (Ø¨Ø§ ÛŒØ§ Ø¨Ø¯ÙˆÙ† ÙØ§ØµÙ„Ù‡): \")\n",
        "number_result = text_to_number_fa(user_input)\n",
        "persian_number = convert_to_persian_number(number_result)\n",
        "\n",
        "print(\"ÙˆØ±ÙˆØ¯ÛŒ:\", user_input)\n",
        "print(\"Ø®Ø±ÙˆØ¬ÛŒ Ø¹Ø¯Ø¯ÛŒ (ÙØ§Ø±Ø³ÛŒ):\", persian_number)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzpWNDxdKuJO",
        "outputId": "25914afc-f977-48b1-c217-139d8a065bf9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ø¹Ø¯Ø¯ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (Ø¨Ø§ ÛŒØ§ Ø¨Ø¯ÙˆÙ† ÙØ§ØµÙ„Ù‡): Ø³Ù‡Ù…ÛŒÙ„ÛŒÙˆÙ†ÙˆØ³ÛŒØµØ¯Ù¾Ù†Ø¬Ø§Ù‡ÙˆØ³Ù‡\n",
            "ÙˆØ±ÙˆØ¯ÛŒ: Ø³Ù‡Ù…ÛŒÙ„ÛŒÙˆÙ†ÙˆØ³ÛŒØµØ¯Ù¾Ù†Ø¬Ø§Ù‡ÙˆØ³Ù‡\n",
            "Ø®Ø±ÙˆØ¬ÛŒ Ø¹Ø¯Ø¯ÛŒ (ÙØ§Ø±Ø³ÛŒ): Û³Û°Û°Û°Û³ÛµÛ³\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2fawords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6kpJiS9LS-c",
        "outputId": "1c17b970-d751-4232-88fe-53fc4593add9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2fawords\n",
            "  Downloading num2fawords-1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Downloading num2fawords-1.1-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: num2fawords\n",
            "Successfully installed num2fawords-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from num2fawords import words\n",
        "\n",
        "def number_to_farsi_text(number):\n",
        "    try:\n",
        "        number = int(number)\n",
        "        return words(number)\n",
        "    except:\n",
        "        return \"Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ø¹Ø¯Ø¯ ØµØ­ÛŒØ­ Ù…Ø¹ØªØ¨Ø± ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.\"\n",
        "\n",
        "# Ú¯Ø±ÙØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² Ú©Ø§Ø±Ø¨Ø±\n",
        "user_input = input(\"Ù„Ø·ÙØ§Ù‹ Ø¹Ø¯Ø¯ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: \")\n",
        "result = number_to_farsi_text(user_input)\n",
        "print(\"Ø¹Ø¯Ø¯ Ø¨Ù‡ Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYnJIkIxM9je",
        "outputId": "df421292-7c15-46ec-ca01-c109871a1d65"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ù„Ø·ÙØ§Ù‹ Ø¹Ø¯Ø¯ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: 3000353\n",
            "Ø¹Ø¯Ø¯ Ø¨Ù‡ Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ: Ø³Ù‡ Ù…ÛŒÙ„ÛŒÙˆÙ† Ùˆ Ø³ÛŒØµØ¯ Ùˆ Ù¾Ù†Ø¬Ø§Ù‡ Ùˆ Ø³Ù‡\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯ÙˆØ·Ø±ÙÙ‡ :    "
      ],
      "metadata": {
        "id": "t5WInrIoOLsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import Normalizer\n",
        "from num2fawords import words as num_to_words\n",
        "import re\n",
        "\n",
        "# Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„\n",
        "words_to_numbers = {\n",
        "    \"ØµÙØ±\": 0, \"ÛŒÚ©\": 1, \"Ø¯Ùˆ\": 2, \"Ø³Ù‡\": 3, \"Ú†Ù‡Ø§Ø±\": 4, \"Ù¾Ù†Ø¬\": 5,\n",
        "    \"Ø´Ø´\": 6, \"Ù‡ÙØª\": 7, \"Ù‡Ø´Øª\": 8, \"Ù†Ù‡\": 9,\n",
        "    \"Ø¯Ù‡\": 10, \"ÛŒØ§Ø²Ø¯Ù‡\": 11, \"Ø¯ÙˆØ§Ø²Ø¯Ù‡\": 12, \"Ø³ÛŒØ²Ø¯Ù‡\": 13, \"Ú†Ù‡Ø§Ø±Ø¯Ù‡\": 14, \"Ù¾Ø§Ù†Ø²Ø¯Ù‡\": 15,\n",
        "    \"Ø´Ø§Ù†Ø²Ø¯Ù‡\": 16, \"Ù‡ÙØ¯Ù‡\": 17, \"Ù‡Ø¬Ø¯Ù‡\": 18, \"Ù†ÙˆØ²Ø¯Ù‡\": 19,\n",
        "    \"Ø¨ÛŒØ³Øª\": 20, \"Ø³ÛŒ\": 30, \"Ú†Ù‡Ù„\": 40, \"Ù¾Ù†Ø¬Ø§Ù‡\": 50,\n",
        "    \"Ø´ØµØª\": 60, \"Ù‡ÙØªØ§Ø¯\": 70, \"Ù‡Ø´ØªØ§Ø¯\": 80, \"Ù†ÙˆØ¯\": 90,\n",
        "    \"ØµØ¯\": 100, \"Ø¯ÙˆÛŒØ³Øª\": 200, \"Ø³ÛŒØµØ¯\": 300, \"Ú†Ù‡Ø§Ø±ØµØ¯\": 400,\n",
        "    \"Ù¾Ø§Ù†ØµØ¯\": 500, \"Ø´Ø´ØµØ¯\": 600, \"Ù‡ÙØªØµØ¯\": 700, \"Ù‡Ø´ØªØµØ¯\": 800, \"Ù†Ù‡ØµØ¯\": 900\n",
        "}\n",
        "\n",
        "magnitudes = {\n",
        "    \"Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯\": 1000000000,\n",
        "    \"Ù…ÛŒÙ„ÛŒÙˆÙ†\": 1000000,\n",
        "    \"Ù‡Ø²Ø§Ø±\": 1000\n",
        "}\n",
        "\n",
        "all_keywords = list(words_to_numbers.keys()) + list(magnitudes.keys())\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø¯Ø¯ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ\n",
        "def convert_to_persian_number(number):\n",
        "    return str(number).translate(str.maketrans(\"0123456789\", \"Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹\"))\n",
        "\n",
        "# Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ\n",
        "def normalize_input(text):\n",
        "    normalizer = Normalizer()\n",
        "    text = normalizer.normalize(text)\n",
        "    text = text.replace(\"â€Œ\", \"\").replace(\" \", \"\")\n",
        "    pattern = \"(\" + \"|\".join(sorted(all_keywords, key=len, reverse=True)) + \")\"\n",
        "    return re.findall(pattern, text)\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø¹Ø¯Ø¯\n",
        "def text_to_number_fa(text):\n",
        "    words = normalize_input(text)\n",
        "    total = 0\n",
        "    chunk = []\n",
        "\n",
        "    for w in words:\n",
        "        if w in magnitudes:\n",
        "            total += sum(words_to_numbers.get(word, 0) for word in chunk) * magnitudes[w]\n",
        "            chunk = []\n",
        "        else:\n",
        "            chunk.append(w)\n",
        "\n",
        "    if chunk:\n",
        "        total += sum(words_to_numbers.get(word, 0) for word in chunk)\n",
        "\n",
        "    return total\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø¯Ø¯ Ø¨Ù‡ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ\n",
        "def number_to_text_fa(number):\n",
        "    try:\n",
        "        number = int(number)\n",
        "        return num_to_words(number)\n",
        "    except:\n",
        "        return \"Ø¹Ø¯Ø¯ ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª.\"\n",
        "\n",
        "# ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ ÙˆØ±ÙˆØ¯ÛŒ Ùˆ Ø§Ø¬Ø±Ø§\n",
        "def smart_converter(user_input):\n",
        "    user_input = user_input.strip()\n",
        "    if user_input.isdigit():  # Ø§Ú¯Ø± ÙÙ‚Ø· Ø¹Ø¯Ø¯ Ø¨ÙˆØ¯\n",
        "        return number_to_text_fa(user_input)\n",
        "    else:\n",
        "        try:\n",
        "            num = text_to_number_fa(user_input)\n",
        "            return convert_to_persian_number(num)\n",
        "        except:\n",
        "            return \"ÙˆØ±ÙˆØ¯ÛŒ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª.\"\n",
        "\n",
        "# --- Ø§Ø¬Ø±Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ ---\n",
        "user_input = input(\"Ø¹Ø¯Ø¯ÛŒ ÛŒØ§ Ù…ØªÙ† Ø¹Ø¯Ø¯ÛŒ ÙØ§Ø±Ø³ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: \")\n",
        "result = smart_converter(user_input)\n",
        "print(\" Ø®Ø±ÙˆØ¬ÛŒ:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWGldFeuOP6l",
        "outputId": "e9d037f1-97b3-4c8b-b651-eb9c7427f89c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ø¹Ø¯Ø¯ÛŒ ÛŒØ§ Ù…ØªÙ† Ø¹Ø¯Ø¯ÛŒ ÙØ§Ø±Ø³ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: 3000353\n",
            " Ø®Ø±ÙˆØ¬ÛŒ: Ø³Ù‡ Ù…ÛŒÙ„ÛŒÙˆÙ† Ùˆ Ø³ÛŒØµØ¯ Ùˆ Ù¾Ù†Ø¬Ø§Ù‡ Ùˆ Ø³Ù‡\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2words\n",
        "!pip install word2number\n",
        "!pip install gTTS\n",
        "!pip install playsound==1.2.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTyJWelDSzgN",
        "outputId": "7c0a068a-8c03-46f8-b3de-f8033b7965e8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=472ec83dc49167a8ab683645b6db046291ca1c2b3e11cebec33447cfbb4ca07f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.14\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=ae25406a8dfa6d9e7f40447a99ca91587593aef81abbc8bc15a47a5e26fdfe49\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n",
            "Requirement already satisfied: gTTS in /usr/local/lib/python3.11/dist-packages (2.5.4)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2025.1.31)\n",
            "Requirement already satisfied: playsound==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from num2words import num2words\n",
        "from word2number import w2n\n",
        "from gtts import gTTS\n",
        "from playsound import playsound\n",
        "import os\n",
        "\n",
        "# ğŸ§ Ù¾Ø®Ø´ ØµÙˆØª Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ\n",
        "def play_english_audio(text):\n",
        "    try:\n",
        "        tts = gTTS(text=text, lang='en')\n",
        "        filename = \"voice_en.mp3\"\n",
        "        tts.save(filename)\n",
        "        playsound(filename)\n",
        "        os.remove(filename)\n",
        "    except Exception as e:\n",
        "        print(\" Audio Error:\", e)\n",
        "\n",
        "#  ØªØ¨Ø¯ÛŒÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ±ÙˆØ¯ÛŒ\n",
        "def smart_converter_en(user_input):\n",
        "    user_input = user_input.strip()\n",
        "    if user_input.replace(\",\", \"\").replace(\"_\", \"\").isdigit():\n",
        "        number = int(user_input.replace(\",\", \"\").replace(\"_\", \"\"))\n",
        "        result = num2words(number, lang='en')\n",
        "        play_english_audio(result)\n",
        "        return result\n",
        "    else:\n",
        "        try:\n",
        "            number = w2n.word_to_num(user_input)\n",
        "            result = \"{:,}\".format(number)\n",
        "            play_english_audio(result)\n",
        "            return result\n",
        "        except:\n",
        "            return \" Invalid input.\"\n",
        "\n",
        "#  Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡\n",
        "user_input = input(\" Enter a number or its English words:\\n> \")\n",
        "output = smart_converter_en(user_input)\n",
        "print(\" Output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0pW0NoUVbjC",
        "outputId": "88d67609-66c2-4637-e911-e8238554f885"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Enter a number or its English words:\n",
            "> 3000353\n",
            " Audio Error: Namespace Gst not available\n",
            " Output: three million, three hundred and fifty-three\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#   Ø³ÛŒØ³ØªÙ… Ø¬Ø§Ù…Ø¹ ÙØ§Ø±Ø³ÛŒ + Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ + Ø§Ø¹Ø´Ø§Ø±ÛŒ + Ù…Ù†ÙÛŒ :"
      ],
      "metadata": {
        "id": "6Gtu9d8ZXeI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import Normalizer\n",
        "from num2fawords import words as fa_words\n",
        "from word2number import w2n\n",
        "from num2words import num2words\n",
        "import re\n",
        "\n",
        "# ÙØ§Ø±Ø³ÛŒ - Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø¹Ø¯Ø§Ø¯\n",
        "fa_words_to_numbers = {\n",
        "    \"ØµÙØ±\": 0, \"ÛŒÚ©\": 1, \"Ø¯Ùˆ\": 2, \"Ø³Ù‡\": 3, \"Ú†Ù‡Ø§Ø±\": 4, \"Ù¾Ù†Ø¬\": 5,\n",
        "    \"Ø´Ø´\": 6, \"Ù‡ÙØª\": 7, \"Ù‡Ø´Øª\": 8, \"Ù†Ù‡\": 9, \"Ø¯Ù‡\": 10, \"ÛŒØ§Ø²Ø¯Ù‡\": 11,\n",
        "    \"Ø¯ÙˆØ§Ø²Ø¯Ù‡\": 12, \"Ø³ÛŒØ²Ø¯Ù‡\": 13, \"Ú†Ù‡Ø§Ø±Ø¯Ù‡\": 14, \"Ù¾Ø§Ù†Ø²Ø¯Ù‡\": 15,\n",
        "    \"Ø´Ø§Ù†Ø²Ø¯Ù‡\": 16, \"Ù‡ÙØ¯Ù‡\": 17, \"Ù‡Ø¬Ø¯Ù‡\": 18, \"Ù†ÙˆØ²Ø¯Ù‡\": 19,\n",
        "    \"Ø¨ÛŒØ³Øª\": 20, \"Ø³ÛŒ\": 30, \"Ú†Ù‡Ù„\": 40, \"Ù¾Ù†Ø¬Ø§Ù‡\": 50,\n",
        "    \"Ø´ØµØª\": 60, \"Ù‡ÙØªØ§Ø¯\": 70, \"Ù‡Ø´ØªØ§Ø¯\": 80, \"Ù†ÙˆØ¯\": 90,\n",
        "    \"ØµØ¯\": 100, \"Ø¯ÙˆÛŒØ³Øª\": 200, \"Ø³ÛŒØµØ¯\": 300, \"Ú†Ù‡Ø§Ø±ØµØ¯\": 400,\n",
        "    \"Ù¾Ø§Ù†ØµØ¯\": 500, \"Ø´Ø´ØµØ¯\": 600, \"Ù‡ÙØªØµØ¯\": 700, \"Ù‡Ø´ØªØµØ¯\": 800, \"Ù†Ù‡ØµØ¯\": 900\n",
        "}\n",
        "fa_magnitudes = {\n",
        "    \"Ù‡Ø²Ø§Ø±\": 1000, \"Ù…ÛŒÙ„ÛŒÙˆÙ†\": 10**6, \"Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯\": 10**9\n",
        "}\n",
        "fa_all_keywords = list(fa_words_to_numbers.keys()) + list(fa_magnitudes.keys()) + [\"Ù…Ù†ÙÛŒ\", \"Ù…Ù…ÛŒØ²\"]\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø¹Ø¯Ø¯\n",
        "def fa_text_to_number(text):\n",
        "    normalizer = Normalizer()\n",
        "    text = normalizer.normalize(text).replace(\"â€Œ\", \"\").replace(\" \", \"\")\n",
        "    pattern = \"(\" + \"|\".join(sorted(fa_all_keywords, key=len, reverse=True)) + \")\"\n",
        "    tokens = re.findall(pattern, text)\n",
        "\n",
        "    total = 0\n",
        "    chunk = []\n",
        "    negative = False\n",
        "    decimal_part = 0.0\n",
        "    is_decimal = False\n",
        "    decimal_chunk = []\n",
        "\n",
        "    for word in tokens:\n",
        "        if word == \"Ù…Ù†ÙÛŒ\":\n",
        "            negative = True\n",
        "        elif word == \"Ù…Ù…ÛŒØ²\":\n",
        "            is_decimal = True\n",
        "        elif word in fa_magnitudes:\n",
        "            chunk_val = sum(fa_words_to_numbers.get(w, 0) for w in chunk)\n",
        "            total += chunk_val * fa_magnitudes[word]\n",
        "            chunk = []\n",
        "        elif is_decimal:\n",
        "            decimal_chunk.append(str(fa_words_to_numbers.get(word, 0)))\n",
        "        else:\n",
        "            chunk.append(word)\n",
        "\n",
        "    total += sum(fa_words_to_numbers.get(w, 0) for w in chunk)\n",
        "    if decimal_chunk:\n",
        "        decimal_part = float(\"0.\" + \"\".join(decimal_chunk))\n",
        "    final = total + decimal_part\n",
        "    return -final if negative else final\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø¯Ø¯ Ø¨Ù‡ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ\n",
        "def fa_number_to_text(n):\n",
        "    try:\n",
        "        n = float(n)\n",
        "        if n < 0:\n",
        "            return \"Ù…Ù†ÙÛŒ \" + fa_words(abs(int(n)))  # Ø¨Ø¯ÙˆÙ† Ø§Ø¹Ø´Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†\n",
        "        else:\n",
        "            return fa_words(int(n))\n",
        "    except:\n",
        "        return \"Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø¯Ø¯ ÙØ§Ø±Ø³ÛŒ\"\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ Ø¹Ø¯Ø¯ (Ø­Ù…Ø§ÛŒØª Ø§Ø² Ø§Ø¹Ø´Ø§Ø± Ùˆ Ù…Ù†ÙÛŒ)\n",
        "def en_text_to_number(text):\n",
        "    try:\n",
        "        text = text.lower().replace(\"minus\", \"negative\")\n",
        "        text = text.replace(\"point\", \".\")\n",
        "        if \"negative\" in text:\n",
        "            text = text.replace(\"negative\", \"\").strip()\n",
        "            return -float(w2n.word_to_num(text))\n",
        "        return float(w2n.word_to_num(text))\n",
        "    except:\n",
        "        return \" Invalid English input\"\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø¯Ø¯ Ø¨Ù‡ Ù…ØªÙ† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ\n",
        "def en_number_to_text(n):\n",
        "    try:\n",
        "        n = float(n)\n",
        "        if n < 0:\n",
        "            return \"minus \" + num2words(abs(n), lang='en')\n",
        "        return num2words(n, lang='en')\n",
        "    except:\n",
        "        return \" Invalid number\"\n",
        "\n",
        "# ØªØ´Ø®ÛŒØµ ÙØ§Ø±Ø³ÛŒ ÛŒØ§ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ\n",
        "def detect_lang(text):\n",
        "    farsi_chars = \"Ø¢Ø§Ø¨Ù¾ØªØ«Ø¬Ú†Ø­Ø®Ø¯Ø°Ø±Ø²Ú˜Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚Ú©Ú¯Ù„Ù…Ù†ÙˆÙ‡ÛŒ\"\n",
        "    return any(ch in text for ch in farsi_chars)\n",
        "\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯ÙˆØ·Ø±ÙÙ‡ Ùˆ Ø¯Ùˆ Ø²Ø¨Ø§Ù†Ù‡\n",
        "def smart_converter(text):\n",
        "    text = text.strip()\n",
        "    is_farsi = detect_lang(text)\n",
        "\n",
        "    if text.replace(\",\", \"\").replace(\".\", \"\").replace(\"-\", \"\").isdigit():\n",
        "        num = float(text)\n",
        "        return fa_number_to_text(num) if is_farsi else en_number_to_text(num)\n",
        "\n",
        "    try:\n",
        "        if is_farsi:\n",
        "            num = fa_text_to_number(text)\n",
        "            return f\"{num:,}\"\n",
        "        else:\n",
        "            num = en_text_to_number(text)\n",
        "            return f\"{num:,}\"\n",
        "    except:\n",
        "        return \" ÙˆØ±ÙˆØ¯ÛŒ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª\"\n",
        "\n",
        "# --- Ø§Ø¬Ø±Ø§ ---\n",
        "user_input = input(\" Ø¹Ø¯Ø¯ ÛŒØ§ Ù…ØªÙ† Ø¹Ø¯Ø¯ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (ÙØ§Ø±Ø³ÛŒ ÛŒØ§ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ):\\n> \")\n",
        "result = smart_converter(user_input)\n",
        "print(\" Ø®Ø±ÙˆØ¬ÛŒ:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_L8bMMjXYKe",
        "outputId": "202c990a-262c-44b3-c25c-a08ee0f769f2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Ø¹Ø¯Ø¯ ÛŒØ§ Ù…ØªÙ† Ø¹Ø¯Ø¯ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (ÙØ§Ø±Ø³ÛŒ ÛŒØ§ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ):\n",
            "> Ù…Ù†ÙÛŒ Ú†Ù‡Ø§Ø±ØµØ¯ Ùˆ Ù‡ÙØªØ§Ø¯ Ùˆ Ù¾Ù†Ø¬ Ù‡Ø²Ø§Ø± Ùˆ Ø¯Ù‡\t\n",
            " Ø®Ø±ÙˆØ¬ÛŒ: -475,010.0\n"
          ]
        }
      ]
    }
  ]
}