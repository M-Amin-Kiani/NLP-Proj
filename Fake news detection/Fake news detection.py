# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ZWRnh_bxHtf_YKqIa7qHtL0Ss5iEkDp
"""

!pip install scikit-learn

!pip install nltk wordcloud matplotlib
import nltk
# nltk.download('all')

# Download necessary NLTK components
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')  # for Lemm

# وارد کردن کتابخانه‌ها
import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


print("All packages are successfully installed!")

from google.colab import drive
drive.mount('/content/drive')

true_path = '/content/drive/MyDrive/True.csv' #/content/drive/MyDrive/True.csv
fake_path = '/content/drive/MyDrive/Fake.csv' #/content/drive/MyDrive/Fake.csv

import nltk
import os

# تعیین مسیر داده‌ها به صورت دستی
nltk_data_path = '/root/nltk_data'
if not os.path.exists(nltk_data_path):
    os.makedirs(nltk_data_path)

nltk.data.path.append(nltk_data_path)

# دانلود مجدد منابع
nltk.download('punkt', download_dir=nltk_data_path)
nltk.download('stopwords', download_dir=nltk_data_path)

!pip install --upgrade tensorflow

!pip install --upgrade numpy

# !pip uninstall -y numpy tensorflow tensorflow-text gensim hazm tf-keras numba
# !pip install numpy==1.26.0 tensorflow==2.18.0 tensorflow-text==2.18.0
# !pip show numpy tensorflow tensorflow-text
# !pip install gensim==4.3.3 hazm==0.10.0 numba==0.60.0

"""# Texts-to-Sequences on Text"""

# وارد کردن کتابخانه‌های مورد نیاز از Keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 1. خواندن داده‌ها
df_true = pd.read_csv(true_path)
df_fake = pd.read_csv(fake_path)

# 2. برچسب‌گذاری
df_true["label"] = 1
df_fake["label"] = 0
df = pd.concat([df_true, df_fake], axis=0)
df = df.sample(frac=1).reset_index(drop=True)  # مخلوط کردن داده‌ها

# 3. انتخاب فقط ستون 'text' (در صورت وجود)
text_column = "text" if "text" in df.columns else df.columns[0]

# =========================================================================================================

# 4. پیش‌پردازش متن با استفاده از Keras Tokenizer
def clean_text(text):
    # 1. تبدیل حروف بزرگ به حروف کوچک
    text = text.lower()

    # 2. حذف فضاهای خالی اضافی
    text = ' '.join(text.split())

    # 3. تجزیه متن به جملات
    sentences = text.split(".")  # ساده‌ترین روش تقسیم جملات با نقطه

    words = []
    stop_words = set(nltk.corpus.stopwords.words('english'))

    for sentence in sentences:
        # 4. توکن‌بندی جملات به کلمات (با استفاده از str.split)
        tokens = sentence.split()

        # 5. حذف اعداد و URLها
        tokens = [word for word in tokens if not word.isdigit() and not word.startswith('http')]

        # 6. حذف علائم نگارشی و ایست‌واژه‌ها
        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

        words.extend(tokens)

    return " ".join(words)

# اعمال پیش‌پردازش روی داده‌ها
df["clean_text"] = df[text_column].astype(str).apply(clean_text)

# =========================================================================================================

# 5. تقسیم داده‌ها
X_train_val, X_test, y_train_val, y_test = train_test_split(df["clean_text"], df["label"], test_size=0.15, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.176, random_state=42)  # 0.176×0.85 ≈ 0.15

# =========================================================================================================

# 6. استفاده از Keras Tokenizer برای تبدیل متن به دنباله‌های عددی
tokenizer = Tokenizer(num_words=5000)  # تعداد کلمات را محدود به 5000 کلمه می‌کنیم
tokenizer.fit_on_texts(X_train)  # آموزش Tokenizer روی داده‌های آموزشی

X_train_seq = tokenizer.texts_to_sequences(X_train)  # تبدیل متن به دنباله‌های عددی
X_val_seq = tokenizer.texts_to_sequences(X_val)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# =========================================================================================================

# 7. Padding دنباله‌ها برای یکسان کردن طول دنباله‌ها
max_length = max([len(seq) for seq in X_train_seq])  # طول دنباله‌ها را پیدا می‌کنیم (از طول بزرگترین دنباله استفاده می‌شود)

X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')
X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')

# =========================================================================================================

# 8. آموزش مدل KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_pad, y_train)

# 9. ارزیابی مدل KNN
y_val_pred_knn = knn.predict(X_val_pad)
print("🔷 KNN on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_knn))
print("Precision:", precision_score(y_val, y_val_pred_knn))
print("Recall:", recall_score(y_val, y_val_pred_knn))
print("F1 Score:", f1_score(y_val, y_val_pred_knn))

# =========================================================================================================

# 10. آموزش مدل SVM
svm = SVC()
svm.fit(X_train_pad, y_train)

# 11. ارزیابی مدل SVM
y_val_pred_svm = svm.predict(X_val_pad)
print("\n🔷 SVM on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_svm))
print("Precision:", precision_score(y_val, y_val_pred_svm))
print("Recall:", recall_score(y_val, y_val_pred_svm))
print("F1 Score:", f1_score(y_val, y_val_pred_svm))

# =========================================================================================================

# 12. ارزیابی نهایی روی Test
y_test_pred_knn = knn.predict(X_test_pad)
y_test_pred_svm = svm.predict(X_test_pad)

print("\n📊 Final Comparison on Test Data:")
print("Model\tAccuracy\tPrecision\tRecall\t\tF1 Score")
print(f"KNN\t{accuracy_score(y_test, y_test_pred_knn):.4f}\t\t{precision_score(y_test, y_test_pred_knn):.4f}\t\t{recall_score(y_test, y_test_pred_knn):.4f}\t\t{f1_score(y_test, y_test_pred_knn):.4f}")
print(f"SVM\t{accuracy_score(y_test, y_test_pred_svm):.4f}\t\t{precision_score(y_test, y_test_pred_svm):.4f}\t\t{recall_score(y_test, y_test_pred_svm):.4f}\t\t{f1_score(y_test, y_test_pred_svm):.4f}")

"""# TF-IDF on Text"""

# 1. خواندن داده‌ها
df_true = pd.read_csv(true_path)
df_fake = pd.read_csv(fake_path)

# 2. برچسب‌گذاری
df_true["label"] = 1
df_fake["label"] = 0
df = pd.concat([df_true, df_fake], axis=0)
df = df.sample(frac=1).reset_index(drop=True)  # مخلوط کردن داده‌ها

# 3. انتخاب فقط ستون 'text' (در صورت وجود)
text_column = "text" if "text" in df.columns else df.columns[0]

# =========================================================================================================

# 4. پیش‌پردازش متن
# def clean_text(text):
#     text = text.lower()  # حروف کوچک
#     text = re.sub(r"http\S+", "", text)  # حذف URL
#     text = re.sub(r"\d+", "", text)      # حذف اعداد
#     text = re.sub(r"[^\w\s]", "", text)  # حذف علائم نگارشی
#     tokens = word_tokenize(text)
#     stop_words = set(stopwords.words('english'))
#     tokens = [word for word in tokens if word not in stop_words]
#     return " ".join(tokens)

# df["clean_text"] = df[text_column].astype(str).apply(clean_text)

# # پیش‌پردازش متن با استفاده از str.split() به جای word_tokenize
# def clean_text(text):
#     text = text.lower()  # حروف کوچک
#     text = re.sub(r"http\S+", "", text)  # حذف URL
#     text = re.sub(r"\d+", "", text)      # حذف اعداد
#     text = re.sub(r"[^\w\s]", "", text)  # حذف علائم نگارشی
#     tokens = text.split()  # استفاده از str.split() برای توکن‌بندی
#     stop_words = set(stopwords.words('english'))
#     tokens = [word for word in tokens if word not in stop_words]
#     return " ".join(tokens)

# # اعمال پیش‌پردازش روی داده‌ها
# df["clean_text"] = df[text_column].astype(str).apply(clean_text)

# پیش‌پردازش متن با استفاده از NLTK
# def clean_text(text):
#     # 1. تبدیل حروف بزرگ به حروف کوچک
#     text = text.lower()

#     # 2. حذف فضاهای خالی اضافی
#     text = ' '.join(text.split())

#     # 3. تجزیه متن به جملات
#     sentences = nltk.sent_tokenize(text)

#     # 4. توکن‌بندی جملات به کلمات و انجام سایر پردازش‌ها
#     words = []
#     stop_words = set(nltk.corpus.stopwords.words('english'))

#     for sentence in sentences:
#         # 5. توکن‌بندی جملات به کلمات
#         tokens = nltk.word_tokenize(sentence)

#         # 6. حذف اعداد و URLها
#         tokens = [word for word in tokens if not word.isdigit() and not word.startswith('http')]

#         # 7. حذف علائم نگارشی و ایست‌واژه‌ها
#         tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

#         words.extend(tokens)

#     return " ".join(words)

# # اعمال پیش‌پردازش روی داده‌ها
# df["clean_text"] = df[text_column].astype(str).apply(clean_text)

def clean_text(text):
    # 1. تبدیل حروف بزرگ به حروف کوچک
    text = text.lower()

    # 2. حذف فضاهای خالی اضافی
    text = ' '.join(text.split())

    # 3. تجزیه متن به جملات
    sentences = text.split(".")  # ساده‌ترین روش تقسیم جملات با نقطه

    words = []
    stop_words = set(nltk.corpus.stopwords.words('english'))

    for sentence in sentences:
        # 4. توکن‌بندی جملات به کلمات (با استفاده از str.split)
        tokens = sentence.split()

        # 5. حذف اعداد و URLها
        tokens = [word for word in tokens if not word.isdigit() and not word.startswith('http')]

        # 6. حذف علائم نگارشی و ایست‌واژه‌ها
        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

        words.extend(tokens)

    return " ".join(words)

# اعمال پیش‌پردازش روی داده‌ها
df["clean_text"] = df[text_column].astype(str).apply(clean_text)

# =========================================================================================================

# 5. تقسیم داده‌ها
X_train_val, X_test, y_train_val, y_test = train_test_split(df["clean_text"], df["label"], test_size=0.15, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.176, random_state=42)  # 0.176×0.85 ≈ 0.15

# 6. بردار‌سازی متن
vectorizer = TfidfVectorizer(max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_val_vec = vectorizer.transform(X_val)
X_test_vec = vectorizer.transform(X_test)

# 7. آموزش مدل KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_vec, y_train)

# 8. ارزیابی مدل KNN
y_val_pred_knn = knn.predict(X_val_vec)
print("🔷 KNN on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_knn))
print("Precision:", precision_score(y_val, y_val_pred_knn))
print("Recall:", recall_score(y_val, y_val_pred_knn))
print("F1 Score:", f1_score(y_val, y_val_pred_knn))

# 9. آموزش مدل SVM
svm = SVC()
svm.fit(X_train_vec, y_train)

# 10. ارزیابی مدل SVM
y_val_pred_svm = svm.predict(X_val_vec)
print("\n🔷 SVM on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_svm))
print("Precision:", precision_score(y_val, y_val_pred_svm))
print("Recall:", recall_score(y_val, y_val_pred_svm))
print("F1 Score:", f1_score(y_val, y_val_pred_svm))

# 11. ارزیابی نهایی روی Test
y_test_pred_knn = knn.predict(X_test_vec)
y_test_pred_svm = svm.predict(X_test_vec)

print("\n📊 Final Comparison on Test Data:")
print("Model\tAccuracy\tPrecision\tRecall\t\tF1 Score")
print(f"KNN\t{accuracy_score(y_test, y_test_pred_knn):.4f}\t\t{precision_score(y_test, y_test_pred_knn):.4f}\t\t{recall_score(y_test, y_test_pred_knn):.4f}\t\t{f1_score(y_test, y_test_pred_knn):.4f}")
print(f"SVM\t{accuracy_score(y_test, y_test_pred_svm):.4f}\t\t{precision_score(y_test, y_test_pred_svm):.4f}\t\t{recall_score(y_test, y_test_pred_svm):.4f}\t\t{f1_score(y_test, y_test_pred_svm):.4f}")

# 1. خواندن داده‌ها
df_true = pd.read_csv(true_path)
df_fake = pd.read_csv(fake_path)

# 2. برچسب‌گذاری
df_true["label"] = 1
df_fake["label"] = 0
df = pd.concat([df_true, df_fake], axis=0)
df = df.sample(frac=1).reset_index(drop=True)  # مخلوط کردن داده‌ها

# 3. انتخاب فقط ستون 'text' (در صورت وجود)
# text_column = "text" if "text" in df.columns else df.columns[0]

df['text'] = df['title'] + " " + df['text'] + " " + df['subject'] + " " + df['date'].astype(str)  # ترکیب عنوان، متن، موضوع و تاریخ

# =========================================================================================================

# # اعمال پیش‌پردازش روی داده‌ها
# df["clean_text"] = df[text_column].astype(str).apply(clean_text)

def clean_text(text):
    # 1. تبدیل حروف بزرگ به حروف کوچک
    text = text.lower()

    # 2. حذف فضاهای خالی اضافی
    text = ' '.join(text.split())

    # 3. تجزیه متن به جملات
    sentences = text.split(".")  # ساده‌ترین روش تقسیم جملات با نقطه

    words = []
    stop_words = set(nltk.corpus.stopwords.words('english'))

    for sentence in sentences:
        # 4. توکن‌بندی جملات به کلمات (با استفاده از str.split)
        tokens = sentence.split()

        # 5. حذف اعداد و URLها
        tokens = [word for word in tokens if not word.isdigit() and not word.startswith('http')]

        # 6. حذف علائم نگارشی و ایست‌واژه‌ها
        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

        words.extend(tokens)

    return " ".join(words)

# اعمال پیش‌پردازش روی داده‌ها
# df["clean_text"] = df[text_column].astype(str).apply(clean_text)

df["clean_text"] = df['text'].astype(str).apply(clean_text)

# =========================================================================================================

# 5. تقسیم داده‌ها
X_train_val, X_test, y_train_val, y_test = train_test_split(df["clean_text"], df["label"], test_size=0.15, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.176, random_state=42)  # 0.176×0.85 ≈ 0.15

# 6. بردار‌سازی متن
vectorizer = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.95)
# vectorizer = TfidfVectorizer(max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_val_vec = vectorizer.transform(X_val)
X_test_vec = vectorizer.transform(X_test)

# 7. آموزش مدل KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_vec, y_train)

# 8. ارزیابی مدل KNN
y_val_pred_knn = knn.predict(X_val_vec)
print("KNN on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_knn))
print("Precision:", precision_score(y_val, y_val_pred_knn))
print("Recall:", recall_score(y_val, y_val_pred_knn))
print("F1 Score:", f1_score(y_val, y_val_pred_knn))

# 9. آموزش مدل SVM
svm = SVC(kernel='rbf', C=1, gamma='scale')
# svm = SVC()
svm.fit(X_train_vec, y_train)

# 10. ارزیابی مدل SVM
y_val_pred_svm = svm.predict(X_val_vec)
print("\nSVM on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_svm))
print("Precision:", precision_score(y_val, y_val_pred_svm))
print("Recall:", recall_score(y_val, y_val_pred_svm))
print("F1 Score:", f1_score(y_val, y_val_pred_svm))

# 11. ارزیابی نهایی روی Test
y_test_pred_knn = knn.predict(X_test_vec)
y_test_pred_svm = svm.predict(X_test_vec)

print("\nFinal Comparison on Test Data:")
print("Model\tAccuracy\tPrecision\tRecall\t\tF1 Score")
print(f"KNN\t{accuracy_score(y_test, y_test_pred_knn):.4f}\t\t{precision_score(y_test, y_test_pred_knn):.4f}\t\t{recall_score(y_test, y_test_pred_knn):.4f}\t\t{f1_score(y_test, y_test_pred_knn):.4f}")
print(f"SVM\t{accuracy_score(y_test, y_test_pred_svm):.4f}\t\t{precision_score(y_test, y_test_pred_svm):.4f}\t\t{recall_score(y_test, y_test_pred_svm):.4f}\t\t{f1_score(y_test, y_test_pred_svm):.4f}")

"""# Texts-to-Sequences on title, Text, subject, date"""

# وارد کردن کتابخانه‌های مورد نیاز از Keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
import nltk

# 1. خواندن داده‌ها
df_true = pd.read_csv(true_path)
df_fake = pd.read_csv(fake_path)

# 2. برچسب‌گذاری
df_true["label"] = 1
df_fake["label"] = 0
df = pd.concat([df_true, df_fake], axis=0)
df = df.sample(frac=1).reset_index(drop=True)  # مخلوط کردن داده‌ها

# 3. انتخاب ستون‌های اضافی برای بهبود دقت مدل
df['text'] = df['title'] + " " + df['text'] + " " + df['subject'] + " " + df['date'].astype(str)  # ترکیب عنوان، متن، موضوع و تاریخ

# =========================================================================================================

# 4. پیش‌پردازش متن با استفاده از Keras Tokenizer
def clean_text(text):
    # 1. تبدیل حروف بزرگ به حروف کوچک
    text = text.lower()

    # 2. حذف فضاهای خالی اضافی
    text = ' '.join(text.split())

    # 3. تجزیه متن به جملات
    sentences = text.split(".")  # ساده‌ترین روش تقسیم جملات با نقطه

    words = []
    stop_words = set(nltk.corpus.stopwords.words('english'))

    for sentence in sentences:
        # 4. توکن‌بندی جملات به کلمات (با استفاده از str.split)
        tokens = sentence.split()

        # 5. حذف اعداد و URLها
        tokens = [word for word in tokens if not word.isdigit() and not word.startswith('http')]

        # 6. حذف علائم نگارشی و ایست‌واژه‌ها
        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

        words.extend(tokens)

    return " ".join(words)

# اعمال پیش‌پردازش روی داده‌ها
df["clean_text"] = df['text'].astype(str).apply(clean_text)

# =========================================================================================================

# 5. تقسیم داده‌ها
X_train_val, X_test, y_train_val, y_test = train_test_split(df["clean_text"], df["label"], test_size=0.15, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.176, random_state=42)  # 0.176×0.85 ≈ 0.15

# =========================================================================================================

# 6. استفاده از Keras Tokenizer برای تبدیل متن به دنباله‌های عددی
tokenizer = Tokenizer(num_words=5000)  # تعداد کلمات را محدود به 5000 کلمه می‌کنیم
tokenizer.fit_on_texts(X_train)  # آموزش Tokenizer روی داده‌های آموزشی

X_train_seq = tokenizer.texts_to_sequences(X_train)  # تبدیل متن به دنباله‌های عددی
X_val_seq = tokenizer.texts_to_sequences(X_val)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# =========================================================================================================

# 7. Padding دنباله‌ها برای یکسان کردن طول دنباله‌ها
max_length = max([len(seq) for seq in X_train_seq])  # طول دنباله‌ها را پیدا می‌کنیم (از طول بزرگترین دنباله استفاده می‌شود)

X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')
X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')

# =========================================================================================================

# 8. آموزش مدل KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_pad, y_train)

# 9. ارزیابی مدل KNN
y_val_pred_knn = knn.predict(X_val_pad)
print(" _ KNN on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_knn))
print("Precision:", precision_score(y_val, y_val_pred_knn))
print("Recall:", recall_score(y_val, y_val_pred_knn))
print("F1 Score:", f1_score(y_val, y_val_pred_knn))

# =========================================================================================================

# 10. آموزش مدل SVM
svm = SVC()
svm.fit(X_train_pad, y_train)

# 11. ارزیابی مدل SVM
y_val_pred_svm = svm.predict(X_val_pad)
print("\n _ SVM on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_svm))
print("Precision:", precision_score(y_val, y_val_pred_svm))
print("Recall:", recall_score(y_val, y_val_pred_svm))
print("F1 Score:", f1_score(y_val, y_val_pred_svm))

# =========================================================================================================

# 12. ارزیابی نهایی روی Test
y_test_pred_knn = knn.predict(X_test_pad)
y_test_pred_svm = svm.predict(X_test_pad)

print("\n ____Final Comparison on Test Data:")
print("Model\tAccuracy\tPrecision\tRecall\t\tF1 Score")
print(f"KNN\t{accuracy_score(y_test, y_test_pred_knn):.4f}\t\t{precision_score(y_test, y_test_pred_knn):.4f}\t\t{recall_score(y_test, y_test_pred_knn):.4f}\t\t{f1_score(y_test, y_test_pred_knn):.4f}")
print(f"SVM\t{accuracy_score(y_test, y_test_pred_svm):.4f}\t\t{precision_score(y_test, y_test_pred_svm):.4f}\t\t{recall_score(y_test, y_test_pred_svm):.4f}\t\t{f1_score(y_test, y_test_pred_svm):.4f}")