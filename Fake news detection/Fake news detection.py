# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ZWRnh_bxHtf_YKqIa7qHtL0Ss5iEkDp
"""

!pip install scikit-learn

!pip install nltk wordcloud matplotlib
import nltk
# nltk.download('all')

# Download necessary NLTK components
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')  # for Lemm

# ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§
import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


print("All packages are successfully installed!")

from google.colab import drive
drive.mount('/content/drive')

true_path = '/content/drive/MyDrive/True.csv' #/content/drive/MyDrive/True.csv
fake_path = '/content/drive/MyDrive/Fake.csv' #/content/drive/MyDrive/Fake.csv

import nltk
import os

# ØªØ¹ÛŒÛŒÙ† Ù…Ø³ÛŒØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÛŒ
nltk_data_path = '/root/nltk_data'
if not os.path.exists(nltk_data_path):
    os.makedirs(nltk_data_path)

nltk.data.path.append(nltk_data_path)

# Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¬Ø¯Ø¯ Ù…Ù†Ø§Ø¨Ø¹
nltk.download('punkt', download_dir=nltk_data_path)
nltk.download('stopwords', download_dir=nltk_data_path)

!pip install --upgrade tensorflow

!pip install --upgrade numpy

# !pip uninstall -y numpy tensorflow tensorflow-text gensim hazm tf-keras numba
# !pip install numpy==1.26.0 tensorflow==2.18.0 tensorflow-text==2.18.0
# !pip show numpy tensorflow tensorflow-text
# !pip install gensim==4.3.3 hazm==0.10.0 numba==0.60.0

"""# Texts-to-Sequences on Text"""

# ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø§Ø² Keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 1. Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
df_true = pd.read_csv(true_path)
df_fake = pd.read_csv(fake_path)

# 2. Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ
df_true["label"] = 1
df_fake["label"] = 0
df = pd.concat([df_true, df_fake], axis=0)
df = df.sample(frac=1).reset_index(drop=True)  # Ù…Ø®Ù„ÙˆØ· Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§

# 3. Ø§Ù†ØªØ®Ø§Ø¨ ÙÙ‚Ø· Ø³ØªÙˆÙ† 'text' (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)
text_column = "text" if "text" in df.columns else df.columns[0]

# =========================================================================================================

# 4. Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Keras Tokenizer
def clean_text(text):
    # 1. ØªØ¨Ø¯ÛŒÙ„ Ø­Ø±ÙˆÙ Ø¨Ø²Ø±Ú¯ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©
    text = text.lower()

    # 2. Ø­Ø°Ù ÙØ¶Ø§Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø§Ø¶Ø§ÙÛŒ
    text = ' '.join(text.split())

    # 3. ØªØ¬Ø²ÛŒÙ‡ Ù…ØªÙ† Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª
    sentences = text.split(".")  # Ø³Ø§Ø¯Ù‡â€ŒØªØ±ÛŒÙ† Ø±ÙˆØ´ ØªÙ‚Ø³ÛŒÙ… Ø¬Ù…Ù„Ø§Øª Ø¨Ø§ Ù†Ù‚Ø·Ù‡

    words = []
    stop_words = set(nltk.corpus.stopwords.words('english'))

    for sentence in sentences:
        # 4. ØªÙˆÚ©Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø¬Ù…Ù„Ø§Øª Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª (Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² str.split)
        tokens = sentence.split()

        # 5. Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ URLÙ‡Ø§
        tokens = [word for word in tokens if not word.isdigit() and not word.startswith('http')]

        # 6. Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ Ùˆ Ø§ÛŒØ³Øªâ€ŒÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§
        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

        words.extend(tokens)

    return " ".join(words)

# Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
df["clean_text"] = df[text_column].astype(str).apply(clean_text)

# =========================================================================================================

# 5. ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
X_train_val, X_test, y_train_val, y_test = train_test_split(df["clean_text"], df["label"], test_size=0.15, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.176, random_state=42)  # 0.176Ã—0.85 â‰ˆ 0.15

# =========================================================================================================

# 6. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Keras Tokenizer Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
tokenizer = Tokenizer(num_words=5000)  # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ 5000 Ú©Ù„Ù…Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
tokenizer.fit_on_texts(X_train)  # Ø¢Ù…ÙˆØ²Ø´ Tokenizer Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ

X_train_seq = tokenizer.texts_to_sequences(X_train)  # ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
X_val_seq = tokenizer.texts_to_sequences(X_val)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# =========================================================================================================

# 7. Padding Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÛŒÚ©Ø³Ø§Ù† Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§
max_length = max([len(seq) for seq in X_train_seq])  # Ø·ÙˆÙ„ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (Ø§Ø² Ø·ÙˆÙ„ Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)

X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')
X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')

# =========================================================================================================

# 8. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_pad, y_train)

# 9. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ KNN
y_val_pred_knn = knn.predict(X_val_pad)
print("ğŸ”· KNN on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_knn))
print("Precision:", precision_score(y_val, y_val_pred_knn))
print("Recall:", recall_score(y_val, y_val_pred_knn))
print("F1 Score:", f1_score(y_val, y_val_pred_knn))

# =========================================================================================================

# 10. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ SVM
svm = SVC()
svm.fit(X_train_pad, y_train)

# 11. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ SVM
y_val_pred_svm = svm.predict(X_val_pad)
print("\nğŸ”· SVM on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_svm))
print("Precision:", precision_score(y_val, y_val_pred_svm))
print("Recall:", recall_score(y_val, y_val_pred_svm))
print("F1 Score:", f1_score(y_val, y_val_pred_svm))

# =========================================================================================================

# 12. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Test
y_test_pred_knn = knn.predict(X_test_pad)
y_test_pred_svm = svm.predict(X_test_pad)

print("\nğŸ“Š Final Comparison on Test Data:")
print("Model\tAccuracy\tPrecision\tRecall\t\tF1 Score")
print(f"KNN\t{accuracy_score(y_test, y_test_pred_knn):.4f}\t\t{precision_score(y_test, y_test_pred_knn):.4f}\t\t{recall_score(y_test, y_test_pred_knn):.4f}\t\t{f1_score(y_test, y_test_pred_knn):.4f}")
print(f"SVM\t{accuracy_score(y_test, y_test_pred_svm):.4f}\t\t{precision_score(y_test, y_test_pred_svm):.4f}\t\t{recall_score(y_test, y_test_pred_svm):.4f}\t\t{f1_score(y_test, y_test_pred_svm):.4f}")

"""# TF-IDF on Text"""

# 1. Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
df_true = pd.read_csv(true_path)
df_fake = pd.read_csv(fake_path)

# 2. Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ
df_true["label"] = 1
df_fake["label"] = 0
df = pd.concat([df_true, df_fake], axis=0)
df = df.sample(frac=1).reset_index(drop=True)  # Ù…Ø®Ù„ÙˆØ· Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§

# 3. Ø§Ù†ØªØ®Ø§Ø¨ ÙÙ‚Ø· Ø³ØªÙˆÙ† 'text' (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)
text_column = "text" if "text" in df.columns else df.columns[0]

# =========================================================================================================

# 4. Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†
# def clean_text(text):
#     text = text.lower()  # Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©
#     text = re.sub(r"http\S+", "", text)  # Ø­Ø°Ù URL
#     text = re.sub(r"\d+", "", text)      # Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯
#     text = re.sub(r"[^\w\s]", "", text)  # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ
#     tokens = word_tokenize(text)
#     stop_words = set(stopwords.words('english'))
#     tokens = [word for word in tokens if word not in stop_words]
#     return " ".join(tokens)

# df["clean_text"] = df[text_column].astype(str).apply(clean_text)

# # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² str.split() Ø¨Ù‡ Ø¬Ø§ÛŒ word_tokenize
# def clean_text(text):
#     text = text.lower()  # Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©
#     text = re.sub(r"http\S+", "", text)  # Ø­Ø°Ù URL
#     text = re.sub(r"\d+", "", text)      # Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯
#     text = re.sub(r"[^\w\s]", "", text)  # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ
#     tokens = text.split()  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² str.split() Ø¨Ø±Ø§ÛŒ ØªÙˆÚ©Ù†â€ŒØ¨Ù†Ø¯ÛŒ
#     stop_words = set(stopwords.words('english'))
#     tokens = [word for word in tokens if word not in stop_words]
#     return " ".join(tokens)

# # Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
# df["clean_text"] = df[text_column].astype(str).apply(clean_text)

# Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² NLTK
# def clean_text(text):
#     # 1. ØªØ¨Ø¯ÛŒÙ„ Ø­Ø±ÙˆÙ Ø¨Ø²Ø±Ú¯ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©
#     text = text.lower()

#     # 2. Ø­Ø°Ù ÙØ¶Ø§Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø§Ø¶Ø§ÙÛŒ
#     text = ' '.join(text.split())

#     # 3. ØªØ¬Ø²ÛŒÙ‡ Ù…ØªÙ† Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª
#     sentences = nltk.sent_tokenize(text)

#     # 4. ØªÙˆÚ©Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø¬Ù…Ù„Ø§Øª Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø³Ø§ÛŒØ± Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒÙ‡Ø§
#     words = []
#     stop_words = set(nltk.corpus.stopwords.words('english'))

#     for sentence in sentences:
#         # 5. ØªÙˆÚ©Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø¬Ù…Ù„Ø§Øª Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª
#         tokens = nltk.word_tokenize(sentence)

#         # 6. Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ URLÙ‡Ø§
#         tokens = [word for word in tokens if not word.isdigit() and not word.startswith('http')]

#         # 7. Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ Ùˆ Ø§ÛŒØ³Øªâ€ŒÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§
#         tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

#         words.extend(tokens)

#     return " ".join(words)

# # Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
# df["clean_text"] = df[text_column].astype(str).apply(clean_text)

def clean_text(text):
    # 1. ØªØ¨Ø¯ÛŒÙ„ Ø­Ø±ÙˆÙ Ø¨Ø²Ø±Ú¯ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©
    text = text.lower()

    # 2. Ø­Ø°Ù ÙØ¶Ø§Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø§Ø¶Ø§ÙÛŒ
    text = ' '.join(text.split())

    # 3. ØªØ¬Ø²ÛŒÙ‡ Ù…ØªÙ† Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª
    sentences = text.split(".")  # Ø³Ø§Ø¯Ù‡â€ŒØªØ±ÛŒÙ† Ø±ÙˆØ´ ØªÙ‚Ø³ÛŒÙ… Ø¬Ù…Ù„Ø§Øª Ø¨Ø§ Ù†Ù‚Ø·Ù‡

    words = []
    stop_words = set(nltk.corpus.stopwords.words('english'))

    for sentence in sentences:
        # 4. ØªÙˆÚ©Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø¬Ù…Ù„Ø§Øª Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª (Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² str.split)
        tokens = sentence.split()

        # 5. Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ URLÙ‡Ø§
        tokens = [word for word in tokens if not word.isdigit() and not word.startswith('http')]

        # 6. Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ Ùˆ Ø§ÛŒØ³Øªâ€ŒÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§
        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

        words.extend(tokens)

    return " ".join(words)

# Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
df["clean_text"] = df[text_column].astype(str).apply(clean_text)

# =========================================================================================================

# 5. ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
X_train_val, X_test, y_train_val, y_test = train_test_split(df["clean_text"], df["label"], test_size=0.15, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.176, random_state=42)  # 0.176Ã—0.85 â‰ˆ 0.15

# 6. Ø¨Ø±Ø¯Ø§Ø±â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†
vectorizer = TfidfVectorizer(max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_val_vec = vectorizer.transform(X_val)
X_test_vec = vectorizer.transform(X_test)

# 7. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_vec, y_train)

# 8. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ KNN
y_val_pred_knn = knn.predict(X_val_vec)
print("ğŸ”· KNN on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_knn))
print("Precision:", precision_score(y_val, y_val_pred_knn))
print("Recall:", recall_score(y_val, y_val_pred_knn))
print("F1 Score:", f1_score(y_val, y_val_pred_knn))

# 9. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ SVM
svm = SVC()
svm.fit(X_train_vec, y_train)

# 10. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ SVM
y_val_pred_svm = svm.predict(X_val_vec)
print("\nğŸ”· SVM on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_svm))
print("Precision:", precision_score(y_val, y_val_pred_svm))
print("Recall:", recall_score(y_val, y_val_pred_svm))
print("F1 Score:", f1_score(y_val, y_val_pred_svm))

# 11. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Test
y_test_pred_knn = knn.predict(X_test_vec)
y_test_pred_svm = svm.predict(X_test_vec)

print("\nğŸ“Š Final Comparison on Test Data:")
print("Model\tAccuracy\tPrecision\tRecall\t\tF1 Score")
print(f"KNN\t{accuracy_score(y_test, y_test_pred_knn):.4f}\t\t{precision_score(y_test, y_test_pred_knn):.4f}\t\t{recall_score(y_test, y_test_pred_knn):.4f}\t\t{f1_score(y_test, y_test_pred_knn):.4f}")
print(f"SVM\t{accuracy_score(y_test, y_test_pred_svm):.4f}\t\t{precision_score(y_test, y_test_pred_svm):.4f}\t\t{recall_score(y_test, y_test_pred_svm):.4f}\t\t{f1_score(y_test, y_test_pred_svm):.4f}")

# 1. Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
df_true = pd.read_csv(true_path)
df_fake = pd.read_csv(fake_path)

# 2. Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ
df_true["label"] = 1
df_fake["label"] = 0
df = pd.concat([df_true, df_fake], axis=0)
df = df.sample(frac=1).reset_index(drop=True)  # Ù…Ø®Ù„ÙˆØ· Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§

# 3. Ø§Ù†ØªØ®Ø§Ø¨ ÙÙ‚Ø· Ø³ØªÙˆÙ† 'text' (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)
# text_column = "text" if "text" in df.columns else df.columns[0]

df['text'] = df['title'] + " " + df['text'] + " " + df['subject'] + " " + df['date'].astype(str)  # ØªØ±Ú©ÛŒØ¨ Ø¹Ù†ÙˆØ§Ù†ØŒ Ù…ØªÙ†ØŒ Ù…ÙˆØ¶ÙˆØ¹ Ùˆ ØªØ§Ø±ÛŒØ®

# =========================================================================================================

# # Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
# df["clean_text"] = df[text_column].astype(str).apply(clean_text)

def clean_text(text):
    # 1. ØªØ¨Ø¯ÛŒÙ„ Ø­Ø±ÙˆÙ Ø¨Ø²Ø±Ú¯ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©
    text = text.lower()

    # 2. Ø­Ø°Ù ÙØ¶Ø§Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø§Ø¶Ø§ÙÛŒ
    text = ' '.join(text.split())

    # 3. ØªØ¬Ø²ÛŒÙ‡ Ù…ØªÙ† Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª
    sentences = text.split(".")  # Ø³Ø§Ø¯Ù‡â€ŒØªØ±ÛŒÙ† Ø±ÙˆØ´ ØªÙ‚Ø³ÛŒÙ… Ø¬Ù…Ù„Ø§Øª Ø¨Ø§ Ù†Ù‚Ø·Ù‡

    words = []
    stop_words = set(nltk.corpus.stopwords.words('english'))

    for sentence in sentences:
        # 4. ØªÙˆÚ©Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø¬Ù…Ù„Ø§Øª Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª (Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² str.split)
        tokens = sentence.split()

        # 5. Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ URLÙ‡Ø§
        tokens = [word for word in tokens if not word.isdigit() and not word.startswith('http')]

        # 6. Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ Ùˆ Ø§ÛŒØ³Øªâ€ŒÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§
        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

        words.extend(tokens)

    return " ".join(words)

# Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
# df["clean_text"] = df[text_column].astype(str).apply(clean_text)

df["clean_text"] = df['text'].astype(str).apply(clean_text)

# =========================================================================================================

# 5. ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
X_train_val, X_test, y_train_val, y_test = train_test_split(df["clean_text"], df["label"], test_size=0.15, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.176, random_state=42)  # 0.176Ã—0.85 â‰ˆ 0.15

# 6. Ø¨Ø±Ø¯Ø§Ø±â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†
vectorizer = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.95)
# vectorizer = TfidfVectorizer(max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_val_vec = vectorizer.transform(X_val)
X_test_vec = vectorizer.transform(X_test)

# 7. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_vec, y_train)

# 8. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ KNN
y_val_pred_knn = knn.predict(X_val_vec)
print("KNN on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_knn))
print("Precision:", precision_score(y_val, y_val_pred_knn))
print("Recall:", recall_score(y_val, y_val_pred_knn))
print("F1 Score:", f1_score(y_val, y_val_pred_knn))

# 9. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ SVM
svm = SVC(kernel='rbf', C=1, gamma='scale')
# svm = SVC()
svm.fit(X_train_vec, y_train)

# 10. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ SVM
y_val_pred_svm = svm.predict(X_val_vec)
print("\nSVM on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_svm))
print("Precision:", precision_score(y_val, y_val_pred_svm))
print("Recall:", recall_score(y_val, y_val_pred_svm))
print("F1 Score:", f1_score(y_val, y_val_pred_svm))

# 11. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Test
y_test_pred_knn = knn.predict(X_test_vec)
y_test_pred_svm = svm.predict(X_test_vec)

print("\nFinal Comparison on Test Data:")
print("Model\tAccuracy\tPrecision\tRecall\t\tF1 Score")
print(f"KNN\t{accuracy_score(y_test, y_test_pred_knn):.4f}\t\t{precision_score(y_test, y_test_pred_knn):.4f}\t\t{recall_score(y_test, y_test_pred_knn):.4f}\t\t{f1_score(y_test, y_test_pred_knn):.4f}")
print(f"SVM\t{accuracy_score(y_test, y_test_pred_svm):.4f}\t\t{precision_score(y_test, y_test_pred_svm):.4f}\t\t{recall_score(y_test, y_test_pred_svm):.4f}\t\t{f1_score(y_test, y_test_pred_svm):.4f}")

"""# Texts-to-Sequences on title, Text, subject, date"""

# ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø§Ø² Keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
import nltk

# 1. Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
df_true = pd.read_csv(true_path)
df_fake = pd.read_csv(fake_path)

# 2. Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ
df_true["label"] = 1
df_fake["label"] = 0
df = pd.concat([df_true, df_fake], axis=0)
df = df.sample(frac=1).reset_index(drop=True)  # Ù…Ø®Ù„ÙˆØ· Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§

# 3. Ø§Ù†ØªØ®Ø§Ø¨ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ù‚Øª Ù…Ø¯Ù„
df['text'] = df['title'] + " " + df['text'] + " " + df['subject'] + " " + df['date'].astype(str)  # ØªØ±Ú©ÛŒØ¨ Ø¹Ù†ÙˆØ§Ù†ØŒ Ù…ØªÙ†ØŒ Ù…ÙˆØ¶ÙˆØ¹ Ùˆ ØªØ§Ø±ÛŒØ®

# =========================================================================================================

# 4. Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Keras Tokenizer
def clean_text(text):
    # 1. ØªØ¨Ø¯ÛŒÙ„ Ø­Ø±ÙˆÙ Ø¨Ø²Ø±Ú¯ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©
    text = text.lower()

    # 2. Ø­Ø°Ù ÙØ¶Ø§Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø§Ø¶Ø§ÙÛŒ
    text = ' '.join(text.split())

    # 3. ØªØ¬Ø²ÛŒÙ‡ Ù…ØªÙ† Ø¨Ù‡ Ø¬Ù…Ù„Ø§Øª
    sentences = text.split(".")  # Ø³Ø§Ø¯Ù‡â€ŒØªØ±ÛŒÙ† Ø±ÙˆØ´ ØªÙ‚Ø³ÛŒÙ… Ø¬Ù…Ù„Ø§Øª Ø¨Ø§ Ù†Ù‚Ø·Ù‡

    words = []
    stop_words = set(nltk.corpus.stopwords.words('english'))

    for sentence in sentences:
        # 4. ØªÙˆÚ©Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ø¬Ù…Ù„Ø§Øª Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª (Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² str.split)
        tokens = sentence.split()

        # 5. Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ URLÙ‡Ø§
        tokens = [word for word in tokens if not word.isdigit() and not word.startswith('http')]

        # 6. Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ Ùˆ Ø§ÛŒØ³Øªâ€ŒÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§
        tokens = [word for word in tokens if word.isalpha() and word not in stop_words]

        words.extend(tokens)

    return " ".join(words)

# Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
df["clean_text"] = df['text'].astype(str).apply(clean_text)

# =========================================================================================================

# 5. ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
X_train_val, X_test, y_train_val, y_test = train_test_split(df["clean_text"], df["label"], test_size=0.15, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.176, random_state=42)  # 0.176Ã—0.85 â‰ˆ 0.15

# =========================================================================================================

# 6. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Keras Tokenizer Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
tokenizer = Tokenizer(num_words=5000)  # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ 5000 Ú©Ù„Ù…Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
tokenizer.fit_on_texts(X_train)  # Ø¢Ù…ÙˆØ²Ø´ Tokenizer Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ

X_train_seq = tokenizer.texts_to_sequences(X_train)  # ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
X_val_seq = tokenizer.texts_to_sequences(X_val)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# =========================================================================================================

# 7. Padding Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÛŒÚ©Ø³Ø§Ù† Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§
max_length = max([len(seq) for seq in X_train_seq])  # Ø·ÙˆÙ„ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (Ø§Ø² Ø·ÙˆÙ„ Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯)

X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')
X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')

# =========================================================================================================

# 8. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_pad, y_train)

# 9. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ KNN
y_val_pred_knn = knn.predict(X_val_pad)
print(" _ KNN on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_knn))
print("Precision:", precision_score(y_val, y_val_pred_knn))
print("Recall:", recall_score(y_val, y_val_pred_knn))
print("F1 Score:", f1_score(y_val, y_val_pred_knn))

# =========================================================================================================

# 10. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ SVM
svm = SVC()
svm.fit(X_train_pad, y_train)

# 11. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ SVM
y_val_pred_svm = svm.predict(X_val_pad)
print("\n _ SVM on Validation:")
print("Accuracy:", accuracy_score(y_val, y_val_pred_svm))
print("Precision:", precision_score(y_val, y_val_pred_svm))
print("Recall:", recall_score(y_val, y_val_pred_svm))
print("F1 Score:", f1_score(y_val, y_val_pred_svm))

# =========================================================================================================

# 12. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Test
y_test_pred_knn = knn.predict(X_test_pad)
y_test_pred_svm = svm.predict(X_test_pad)

print("\n ____Final Comparison on Test Data:")
print("Model\tAccuracy\tPrecision\tRecall\t\tF1 Score")
print(f"KNN\t{accuracy_score(y_test, y_test_pred_knn):.4f}\t\t{precision_score(y_test, y_test_pred_knn):.4f}\t\t{recall_score(y_test, y_test_pred_knn):.4f}\t\t{f1_score(y_test, y_test_pred_knn):.4f}")
print(f"SVM\t{accuracy_score(y_test, y_test_pred_svm):.4f}\t\t{precision_score(y_test, y_test_pred_svm):.4f}\t\t{recall_score(y_test, y_test_pred_svm):.4f}\t\t{f1_score(y_test, y_test_pred_svm):.4f}")